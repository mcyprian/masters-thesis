%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  twoside, %% This option enables double-sided typesetting. Use at
           %% least 120 g/m² paper to prevent show-through. Replace
           %% with `oneside` to use one-sided typesetting; use only
           %% if you don’t have access to a double-sided printer,
           %% or if one-sided typesetting is a formal requirement
           %% at your faculty.
  table,   %% This option causes the coloring of tables. Replace
           %% with `notable` to restore plain LaTeX tables.
  lof,     %% This option prints the List of Figures. Replace with
           %% `nolof` to hide the List of Figures.
  lot,     %% This option prints the List of Tables. Replace with
           %% `nolot` to hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = \the\year/\the\month/\the\day,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Michal Cyprian,
    gender        = m,
    advisor       = doc. RNDr. Vlastislav Dohnal Ph.D.,
    title         = {High-availability for PostgreSQL in OpenShift},
    keywords      = {PostgreSQL, OpenShift, Kubernetes, High-availability, Relational Databases, Data Replication, Operator Framework},
    abstract      = {%
      Automated environments such as OpenShift and Kubernetes are widely used for stateless applications, however running stateful systems like databases in the same environments is not well adapted yet. This thesis focuses on the problematic of integrating stateful, highly available database cluster into OpenShift. The solution includes investigation on the existing tools and identification of the threatens and benefits of running databases inside automated cloud environment. The implementation part consists of container images containing selected PostgreSQL high-availability tools and the operator managing these containers in the OpenShift environment. The operator is able to initialize the cluster, detect crashes and perform failover in the autonomous way.
    },
    thanks        = {%
      Firstly, I would like to thank my thesis advisor doc. RNDr. Vlastislav Dohnal, Ph.D. for his support, patience and many pieces of valuable advice. I am also grateful to the people working on the OpenShift components, especially Jan Horák, who shared his technical knowledge and experience with me and was always willing to discuss my ideas and problems, that appeared during my research and design of the solution.
    },
    bib           = citations.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[hyphens]{url} %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{tabularx} %% Tables
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{listings} %% Source code highlighting
\usepackage{graphicx} %% image management
\graphicspath{ {fithesis/images/} }
\lstset{
  basicstyle        = \footnotesize,
  identifierstyle   = \color{black},
  numbers           = left,
  stepnumber        = 1,
  numbersep         = 10pt,
  tabsize           = 1,
  keywordstyle      = \color{blue},
  keywordstyle      = {[2]\color{cyan}},
  keywordstyle      = {[3]\color{olive}},
  stringstyle       = \color{teal},
  commentstyle      = \itshape\color{magenta},
  breaklines        = true,
  breakatwhitespace = false,
  showstringspaces  = false,
  frame             = single,
}

\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
%% The following code fixes the rendering of BibLaTeX ISO 690
%% references in old TeX Live (such as the one at Overleaf).
\thesisload
\makeatletter
\def\thesis@biblatexiso@fix@package{iso-numeric.bbx}
\def\thesis@biblatexiso@fix@end{\relax}
\newif\ifthesis@biblatexiso@fix@
\thesis@biblatexiso@fix@false
\def\thesis@biblatexiso@fix@next#1,{%
  \def\thesis@biblatexiso@fix@current{#1}%
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@package
    \thesis@biblatexiso@fix@true
  \fi
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@end
    \expandafter
    \@gobble
  \fi
  \thesis@biblatexiso@fix@next
}
\expandafter\expandafter\expandafter\thesis@biblatexiso@fix@next\@filelist,\relax,
\ifthesis@biblatexiso@fix@
  \defbibenvironment{bibliography}
    {\list%
       {\MethodFormat}%
       {\setlength{\labelwidth}{\labelnumberwidth}%
        \setlength{\leftmargin}{\labelwidth}%
        \setlength{\labelsep}{\biblabelsep}%
        \addtolength{\leftmargin}{\labelsep}%
        \setlength{\itemsep}{\bibitemsep}%
        \setlength{\parsep}{\bibparsep}}%
        \renewcommand*{\makelabel}[1]{\hss##1}
        }%
    {\endlist}%
  {\item}%
\fi
\makeatother
\begin{document}
\chapter{Introduction}

We live in the computer age. People can access information and knowledge faster and easier than ever before. The evolution of information technology in the last few decades have a great impact on the society. The heavy personal computers have been replaced by tiny devices, laptops, cell phones and smart watches. The performance have been increased, internet connection is available almost everywhere and the number of easy to use applications and web pages for various purposes and have grown significantly.

This revolution also brought a lots of new challenges and rapid changes on the other side of computer industry. All the data centers, where pieces of information are stored, computers, which runs those services, web pages and applications must have been adapted to the requirements of this age equally well.

The main goals of web service providers are to make the service high available, capable of handling big number of requests each second and update running services smoothly. The service should survive disk corruption on some of the computers it is running on, small network issues and sometimes event natural disasters in the area, where data center is located. Ideally, the user shouldn't experience any downtime of the service, when it is being updated to newer version. Software engineers have started an effort to develop complex platforms able to face such challenges in the automated way.

Automated platforms currently work quite well for a simple applications. Additional level of complexity is added when systems storing a specific state, such as databases are managed automatically by the platform. This area is still relatively unexplored and provides a plenty of fascinating design problems to be solved. That is the main reason why I have chosen integrating of high available database setup into one of the automated platform as a topic for my thesis.

The main goal of this thesis is to explore an existing tools and solutions for both, database high availability and the automated platforms, choose the most suitable tools to be integrated into a single system and implement the missing automation logic. The resulting system should be able to handle real world use cases of high available database autonomously. The final part of the thesis is to prepare different scenarios, including misbehaviour and crashes and evaluate behaviour of the system.

The following Chapter \ref{chap:cloud_native} describes recent transformation of the approach to design and manage applications. The importance and core principles of automated platforms for service management, which are necessary for deeper understanding of the thesis topic are included as well.

Chapter \ref{chap:ha_database_systems} provides theoretical background and summarizes current state of the database systems, especially solutions for highly available database systems, which are the second important part of the topic.

The next Chapter \ref{chap:pg_in_openshift} focuses on the design, implementation and testing of the high-availability solution for PostgreSQL database system running in OpenShift platform, based on the research and theoretical knowledge covered in first two chapters.

The process of automated and manual testing of the cluster managed by PostgreSQL operator is covered in Chapter \ref{chap:testing}. Crashes of database system nodes are simulated and the performance and availability of the system in different scenarios is measured. The process of testing and all the results are summarized here. Test results are followed by the final conclusion (Chapter \ref{chap:conclusion}).

\chapter{Cloud native computing} \label{chap:cloud_native}
The new approach to design, create and deploy applications is much different from the traditional way, which includes many manual stages. The main goals are to get new services to the market faster, minimize the cost and increase scalability of the services. Automation of all the stages of service life cycle, using principles like micro services architecture, continuous delivery and container orchestration platforms is the key to achieve the goals. This particular approach to application life cycle combined with new tooling is usually called "Cloud Native" \cite{cloud_native}.

The following chapter describes concrete tools that are the core components of Cloud Native approach.

\section{Linux containers} \label{sec:containers}
The idea of isolating some processes from the rest of operating system has a long history. There are many reasons for isolating part of the system. Sandboxing non trusted application can prevent security threats. Installing application dependencies into a separate file system can help to avoid requirement version conflicts and isolated program alongside its dependencies can be distributed to different machines with lower risk of introducing issues.

The invention of virtual machine is connected with IBM and its effort to share usage of computer resources among groups of users in 1960s \cite{vm_history}. The virtualization concept has evolved into complex technology. The core principle is in running multiple operating systems on a single physical machine. This is usually achieved by making an abstraction of the physical hardware. Virtualization is used heavily in the data centers and also on desktop computers. However, running many operating system is not ideal for some use cases and there were multiple attempts to develop isolation on the operating system level instead, especially on Unix-like systems.

The concept of chroot \cite{chroot} was introduced during the development of Unix Version 7 in 1979. The chroot system call serves to isolate the process by changing the root directory of the new process to the different path in the system. The process isolated by chroot and its children can only access the directories within its own directory tree. This concept is considered to be the first implementation of lightweight isolation. The chroot system call has still its use cases, forty years after its original implementation.

The concept of chroot improved in several ways later served as a base for FreeBSD Jails \cite{freebsd_jails}. The separation implemented by jails is not limited to the file system access. It allows to isolate all the system resources like users, processes and networking subsystem. The administrator can divide the system into several independent units. Each jail can have an IP address and its own configuration.

In 2006 Google engineers launched project called Process Containers. This feature allows partitioning of resources such as CPU time, system memory, disk and network bandwidth into groups and assigning tasks to these groups. The resource limits for a collection of processes can be specified this way. This feature was later renamed to Control Groups (cgroups) and merged into Linux kernel 2.6.24 \cite{cgroups}.

Another important concept in Linux process isolation are the Linux Namespaces \cite{linux_namespaces}. Namespaces provide processes with their own view of the system resources. There are seven kinds of namespaces managing visibility of different resources:
\begin{itemize}
  \item Mount - serves for isolation of mount points, its similar to chroot system call but provides complete isolation
  \item Process ID - the processes within a specific PID namespace can only see processes in the same namespace, multiple nested process trees can exist on the system
  \item User - isolates UID/GID number spaces
  \item Network - allows to create an virtual network stack for each process
  \item UTS - namespace allowing each process to have specific hostname and domain name
  \item IPC - separates interprocess communication resources
  \item Cgroup - isolates cgroup root directory
\end{itemize}

Control Groups alongside namespaces are a fundamental aspects of an operating system level virtualization on Linux. Isolated virtual instances using a single shared Linux kernel are called containers. The container image is a term used for a static file, which includes guest operating system components, system libraries and executable application itself. The name container refers to an instance of container image running as an isolated process.

LXC \cite{lxc}, containerd \cite{containerd}, rkt \cite{rkt}, CRI-O \cite{cri-o} and many others are the implementations of Linux container runtime, based on similar principles which have been introduced since 2008. Not only container runtimes but also other kinds of tools for building, managing and orchestrating containers have been developed since the initial rise of containers. After the initial period, when the technology was relatively unstable and challenging, it finally become mature enough to be widely used in production.

When comparing Linux containers to the concept of virtual machines, the traditional virtualization runs an entire guest operating system in a virtual machine while containers share kernel of the host. Figure \ref{fig:cnt-vs-vm} shows the difference in virtualization layers between Virtual Machine type 1 hypervisor \cite{hypervisors} and Linux containers. Thanks to its lightweight design, containerization provides shorter build and setup times, smaller image size and real-time provisioning and scalability. In the other hand, process level isolation is less secure than full isolation of the guests systems. It is necessary to follow the best practices when working with Linux containers in order to meet the security standards for production systems. In general Linux containers satisfies the requirements of Cloud Native approach workflows better than heavyweight virtualization methods.

\begin{figure}[H]
\caption{Comparison of virtual machines and containers}
\centering
\includegraphics[width=1\textwidth]{cnt-vs-vm}
\label{fig:cnt-vs-vm}
\end{figure}

\section{Container orchestration} \label{sec:orchestration}
Containers as a standardized format containing everything the application needs to run is an efficient basic element for building scalable services. However this is only the first step on the long way to the complex clusters which are often necessary to solve the problem of running a large number of services at global scale. The cluster is often composed of many application instances to handle thousands of request each minute and survive misbehaving of failure of some of the containers. The service itself is usually divided into multiple units, communicating with each other. The application needs to be backed by some kind of data store or database and it can also use caches.

The management and coordination of such cluster includes many different activities. All these containers must be scheduled and started at the right moment, sometimes they must start in a particular order. In case some of the containers fails, another container of the same kind must be scheduled to replace it. The application endpoints must be exposed to the outside world. In case the new version of application is being deployed, the old containers must be replaced in the smart way to minimize the downtime of the application. The term container orchestration in used for coordinating and sequencing the activities in the cluster.

\section{Kubernetes} \label{sec:k8s}
Many companies started inventing container orchestration systems, the platforms to schedule, manage and run containers. Need for similar platforms is connected with the rise of microservice architecture of the applications. Borg \cite{borg} is one of such systems developed at Google to run theirs services in containers. The new project named Kubernetes was started by Google engineers in 2014. The goal was to create an open source container orchestration, which can be adapted by anyone based on the experience gained on Borg.

The quick adaption and success of Kubernetes is described with following words in the book Cloud Native DevOps with Kubernetes:
\begin{quote}
Kubernetes’s rise was meteoric. While other container orchestration systems existed
before Kubernetes, they were commercial products tied to a vendor, and that was
always a barrier to their widespread adoption. With the advent of a truly free and
open source container orchestration, adoption of both containers and Kubernetes grew
at a phenomenal rate.
By late 2017, the orchestration wars were over, and Kubernetes had won. While other
systems are still in use, from now on companies looking to move their infrastructure
to containers only need to target one platform: Kubernetes. \cite[p. 11]{k8s}.
\end{quote}

The key benefits of Kubernetes \cite{k8s_features} besides automation of operational workflows are the high availability, load balancing, auto scaling, self monitoring and self healing. The high availability is achieved thanks to distributing the important parts of the system over multiple nodes (see Section \ref{sec:node}), which can run in different geographical locations. Load balancing together with auto scaling, prevents overloading of services running in Kubernetes. Incoming requests are distributed across a set of containers and in case the set is not big enough according to some monitored metrics, more objects are added automatically. Many objects running in the Kubernetes cluster are monitored and replaced or restarted in case some failures are detected, which makes the platform self healing.

Following subsections describe the fundamental principles of Kubernetes. This container management system is designed to solve different problems, using plenty of highly customizable objects and it is still evolving quite quickly. It is not possible to fully explain the complex structure of Kubernetes in the scope of one section, but it covers the most basic principles necessary to understand the motivation and design. The base objects described in this section are visualized in Figure \ref{fig:k8s_objects}.

\begin{figure}[H]
\caption{Visualization of basic Kubernetes objects}
\centering
\includegraphics[width=1\textwidth]{k8s-objects}
\label{fig:k8s_objects}
\end{figure}

\subsection{Deployment} \label{sec:deployment}
Kubernetes uses many abstractions to be adaptable for many different cases. It is not enough to schedule and run the container. The containerized application can stop working properly if there is a memory error, disk corruption of some other reason. In case the container fails, it must be replaced by another one with the same image, command and configuration. The supervisor, which stores the container specification and is used for periodic checks if all the desired containers are running and responding is called the \textit{deployment}.

\subsection{Pod} \label{sec:pod}
A \textit{pod} is the second fundamental Kubernetes object. It represents a group of containers, which are scheduled together. They also usually share a storage and the IP address. In most cases there is a single container per pod. The deployments defines the desired state of pods, when the deployment is created and a pod satisfying the deployment doesn't exists, it is scheduled and created. Pod is the smallest deployable artifact in Kubernetes.

\subsection{Node} \label{sec:node}
Pods encapsulate containers, which are scheduled and run. The host operating system, with container runtime installed is needed for running containers as explained in Section \ref{sec:containers}. This is the role of \textit{nodes}. A node can be virtual or physical machine containing services necessary to run pods. Distributing pods over multiple nodes is one of the main pillars of high availability in Kubernetes.

If there are not special constraints configured, the Kubernetes scheduler \cite{kube-scheduler} assigns new pod to one of the nodes, based on multiple criteria. The scheduler logic takes into account if candidate node has sufficient resources to run a pod and tries to find node with the highest score based on its local container image cache, resource balance and many other factors.

\subsection{Service} \label{sec:service}
Pods are often short lived and volatile. A pod can crash, it can be restarted or replaced by new pods containing the latest version of the container image or it can be removed by the pod autoscaler \cite{hpa} at some point. This is problematic, because the network connections to the pod can't rely on its IP address, which can change at any moment. Usually there are multiple replicas of the pod with the different addresses. The concept of Kubernetes \textit{service} provides a \
single, consistent IP address and load balances the incoming requests to a set of pods.

\subsection{Persistent volumes} \label{sec:pv}
Each pod has its own file system, but it is ephemeral. At the moment when pod is restarted all the data are lost. This storage is sufficient for the configuration, which is regenerated during the initialization of the pod, but not for data that must survive restart of the pod. Also in case there are multiple containers running in a pod, they might need to share some files.

The \textit{PersistentVolume} (PV) object solves both of these problems, file sharing and storing the data persistently. It is mounted to the pod and it is accessible for the processes running inside its containers. The PV is just an abstraction, a plenty of different underlying storage types are supported. A \textit{PersistentVolumeClaim} (PVC) is a request for storage, that can be consumed by its user. The PVC defines size of the storage and access mode. If the volumes needs to be provisioned dynamically, \textit{StorageClass} \cite{storage-classes} object can be used to specify the provisioner.

\subsection{Resources} \label{sec:resources}
Each pod definition in Kubernetes can contain a resource requests and limit for its containers \cite{mcrc}. The two most frequently used types of resources are \textit{CPU} specified in core units and \textit{memory} specified in bytes.

Specified resource requests of the container are used by Kubernetes scheduler to select the node for a new pod to be scheduled on. In case the container exceed one of the resource limits it can be restarted or terminated.

\subsection{Object organization} \label{sec:obj_organization}
There is usually a plenty of various objects running inside Kubernetes cluster. It is often necessary to divide them into groups and subsets. Kubernetes supports \textit{namespaces} \cite{namespaces}, virtual isolated clusters built on top of single physical cluster. Namespaces serve to divide objects owned by different teams or deployments of different projects.

Objects living within specific namespace also needs to be organized and divided into smaller sets. Concept of Kubernetes \textit{labels} and \textit{selectors} \cite{labels_selectors} are designed for this purpose.

A label is a key/value pair, which can be attached to objects. Each object, for example deployment, pod or service can have multiple labels. A label is not unique, all the objects belonging into a certain group of objects have exactly the same key/value attached. An example of Kubernetes manifest in the following section shows, how labels can be attached to the deployment and all pods satisfying that deployment.

A label selectors are used to identify all objects sharing specific label. A good example is a Kubernetes service, explained above, which provides a consistent IP address for set of pods. Each service has a label selector defined and it is load balancing incoming requests to all pods holding the label from this selector.

\subsection{Manifests} \label{sec:manifest}
Objects, like pods, deployments and services are usually defined, configured and managed by the administrator directly of by automated scripts. The most frequent way of Kubernetes objects definition is to write an description of the object into file following YAML \cite{YAML} standard. Descriptions of Kubernetes objects are called manifests. Kubernetes manifests can be send to the Kubernetes API server \cite{kubernetes-api} using \textit{kubectl} \cite{kubectl} command line tool and are used to create, modify and delete objects in the cluster.

Following example shows a manifest containing definition of a simple Deployment object in YAML format:
\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
\end{lstlisting}

This manifest defines deployment of three desired replicas of Nginx \cite{nginx} container, which expose port 80. Deployment itself and specified replicas have the labels \cite{labels_selectors}, which serve for better organization of the objects in the cluster and divide objects into smaller subsets.

\section{OpenShift} \label{sec:openshift}
OpenShift \cite{rhccos} is a Kubernetes container platform product having a paid support. Openshift uses Kubernetes as a base and builds additional layers of policies, container image streams and supporting software components on top of it. OpenShift brings stricter security policies, integrated CI/CD and web console for cluster management. From version OpenShift 4 there is also an integrated OperatorHub \cite{operatorhub}, storing community operators. These operators become one of the common ways to deploy services on OpenShift and Kubernetes.

Using OpenShift instead of pure open source Kubernetes project components brings also some limitations. Kubernetes supports many Linux distribution, while Openshift can be easily installed only on Red Hat Linux distributions like Red Hat Enterprise Linux (RHEL) \cite{rhel}, CentOS \cite{centos} and a couple more Red Hat alternatives.

\section{Minikube}
Distributed model is the core feature of Kubernetes, all the resource and important data about the desired cluster state are spread across multiple physical or virtual machines. The setup of fully working distributed Kubernetes cluster requires a lot of resources, configuration and maintenance work.

When the Kubernetes applications are being developed locally or needs to be tested, building highly available distributed cluster is sometimes not necessary. Minikube \cite{minikube} is a tool for running single node Kubernetes cluster locally. It support all the major Kubernetes features and it can run on average personal computer or laptop.

There is also an option to run OpenShift cluster locally. Minishift \cite{minishift} is designed similarly to Minikube, it is a lightweight solution that allows running single node cluster containing also all the OpenShift layers explained in the previous section on top of Kubernetes.

\section{Operators} \label{sec:operators}
The main goal of Kubernetes design is automation. The fundamental Kubernetes objects, such as deployments are periodically checking number of existing replicas, theirs health, status and resource usage. In case some issues are detected in the cluster, various operation actions are executed. This mechanism can handle common misbehaving like random process failures or overloading of replicas in the automated way.

If the Kubernetes setup for stateless web application is done properly, the basic objects are usually enough to create robust and stable service, which can work with minimal number of human operational actions. The world of stateful applications brings much more challenges. Working with a database cluster often requires manual operations on top of the standard workloads in Kubernetes.

\subsection{Kubernetes API extensions}
The concept of operators \cite{kubernetes-operator} was introduced by CoreOS \cite{operators} in 2016 aiming to solve the problem of hard integration of stateless applications into Kubernetes. Operators  are the Kubernetes extensions, which serve to include operational domain knowledge into management of complex application. The main idea of operators is to extend Kubernetes API without modifying code of the container orchestration system itself and create a new kind of object. This object has the API similar to basic Kubernetes objects, like pods, but operates on higher level of abstraction.

In case there are some repetitive patterns in the manual operational actions, it means the workflow can be at least partially automated using operator. An operator can for example provide a database cluster, encapsulating the logic of deployment and management by combining many fundamental Kubernetes objects. The operator is periodically checking the state of the cluster in a loop and can react to specific events in the cluster by modifying underlying objects. The common examples of Kubernetes operators are implementations of cluster logging and monitoring or management of a database clusters of different types.

\subsection{Operator Framework} \label{sec:operator_framework}
The Operator Framework \cite{operator-framework} was released two years after introduction of operators. This framework provides an SDK, which allows to build operators without deep knowledge of low level Kubernetes API, and a couple of other tools supporting development of operators. The Operator SDK serves for faster setup of operator project, generates the boilerplate code and provides the libraries for operator API definition, implementation of control loop and a operator testing framework.

The API can be extended by creating a Custom Resource \cite{custom-resources}, supported by Kubernetes. A new Custom Resource is defined by operator author and it exposes an API in form of the custom manifest \ref{sec:manifest}. This manifest can be used by the administrator to create a Custom Resource in the cluster, modify its configuration after the operator is deployed to the cluster. It is similar to the way how deployments (see Section \ref{sec:deployment}) and other Kubernetes objects are managed using manifests. Currently applied manifests are compared to the state of objects in the cluster periodically and operator performs implemented operations based on configuration of the Custom Resource. The operator itself is a pod running in a cluster, holding the logic implemented to manage Custom Resource, monitoring other objects in the cluster and performing programmed operational actions.

The Operator SDK currently supports three types of operators \cite{operatorhub-sdk}. The first option is building the operator as a pure program in Go programming language \cite{golang}. The SDK generates the boilerplate code, an example YAML manifest introduced in Section \ref{sec:manifest} and the endpoint for reconciliation loop. The second option is to create Ansible operator. Ansible \cite{ansible} is a software provisioning and application deployment tool. Operator SDK allows to define an operator in form of Ansible playbook \cite{wwp} and the SDK generates the Go program, which runs the playbook to manage Kubernetes objects. The last supported variant is a Helm operator. Helm \cite{helm} is a package manager for Kubernetes, which supports templates for manifests, creating reproducible builds and installing and managing Kubernetes applications.

\chapter{Highly available database systems} \label{chap:ha_database_systems}
The problems of data redundancy, consistency and atomicity of operations led to an effort to develop dedicated systems for storing and managing the data. Various types of database systems have been evolved over time to satisfy different technical and business use cases. The data loss affects the business significantly in most of the cases. The great importance of the data leads to necessity of implementing high availability for the database systems. Achieving high availability for database systems is more complex compared to stateless systems, because there must be a single source of truth condition satisfied at the same time.

Following sections introduce concept of relational model, database systems based on this model, and a theoretical background for database systems high availability. The second part of this chapter focuses on PostgreSQL and various tools designed for building highly available clusters using this database system.

\section{Relational database systems}
The term relational database was used for the first time by E. F. Codd in his paper \cite{codd_relational_model} titled \textit{A Relational Model of Data for Large Shared Data Banks} from 1970. A new method of data storing and processing is described in this paper. The modern relational databases are still based on similar principles. Relational databases are based on the relational model, which can be explained by following sentences:
\begin{quote}
The relational model uses a collection of tables to represent both, data and the relationships among those data. Each table has multiple columns, and each column has a unique name. Tables are also known as relations. \cite[p. 9]{db}.
\end{quote}

The relational database systems using standardized Structured Query Language (SQL) \cite{sql_standard}pgs become the most commonly used database solutions for web based applications in last decades.

\section{Database replication} \label{sec:db_replication}
When the relational databases are used to store important data, supporting some critical business or research, the database system is required to be highly available and immune to overloading. These requirements are similar to key features of Kubernetes explained in the previous chapter. The solutions bringing this capabilities to database systems are also based mainly on building a database cluster distributed across multiple database nodes. However, stateful databases have some specifics, which must be taken into the account.

In case the database system is composed of multiple nodes (servers), the data must by synchronized between all the nodes. The process of permanent copying of data changes from one node to another is called database replication.

\subsection{Types of replication} \label{sec:types_of_replication}
There are multiple approaches to database replication, each of them has some advantages and downsides as well. There are also various criteria, which can be used to categorize replication types. The publication PostgreSQL Replication - Second Edition \cite{pg} provides an overview of replication types, which are summarized in following paragraphs.

We can distinguish between \textit{single-master} and  \textit{multi-master} replication. More common and traditional way of database cluster design is to have just a single replica, which can handle write requests. The updates from master replica, processing writes is distributed to the rest of replicas, which operates in the read-only mode. In order to prevent overloading of master, the read requests can be redirected to slave (read-only) replicas.

The second option is to allow load balancing also write requests to all nodes in the cluster. The distribution of all the requests can be an advantage, however, this approach adds a lot of complexity to the solution, it is less transparent and some conflicts in data consistency can occur easily.

The second way of classification is to distinguish whether replication is synchronous or asynchronous. In case the transaction can be committed on master and copied to slave with a small delay, it is called asynchronous replication. Slave is usually a little bit behind, but transactions are fast and immediate. The synchronous replication is more strict in terms of consistency. The transaction can't be committed, before the update is copied to at least two nodes in the replicated cluster. This approach guaranties better consistency, but transactions take longer.

There are also various alternatives how to implement copying of the data between nodes. The concept of physical replication means, that the data are copied in binary format to other nodes. The content of master's data directory is replicated to slaves byte by byte. The logical replication in contrast, transfers logical changes based on the replication identity. The advantages of physical replication are an easy setup and scaling up and down.

Physical replication is more mature and standard way of streaming data among nodes. It is usually easier to setup, and it works well for cases when scaling cluster size up and down is needed.

Logical replication, in the other hand is more flexible, it allows to replicate just a selected tables and send different sets of changes to different subscribers. The setup of logical replication is usually more complex and fast scaling is harder. Logical replication is possible across the database versions, which can be beneficial especially for database upgrades.

\subsection{Failover} \label{sec:failover}
High availability of the database is one of the main motivations to use replicated database clusters as it was mentioned at the beginning of Section \ref{sec:db_replication}. There are multiple nodes keeping the current state of the data, which can be located in the different data centers. In case of the software or hardware failure on one of the nodes, there are still several other nodes, which holds the data and can handle incoming requests.

In case of the single-master replication, sometimes also called master-slave setup, there are two different disaster scenarios. When the defect occurs on one of the slave nodes, it can be restarted or turned off and replaced by a new node. The affected slave replica won't be accepting the data increments for some time, but it can pull all the changes and synchronize with the master node after it is recovered. The functionality and performance of the cluster in not affected too much in majority of the cases.

The worse situation is if the data corruption or some kind of software failure occurs on the master node. The usual solution of such scenario is to choose one of the healthy slave replicas and promote it to a new master. This replica starts handling write requests after promotion. All the other nodes, including the old master, in case it is available again, must be informed, which node is a new master. This procedure of promoting one of the slaves to a master node is called the failover.

It is not easy to setup the failover correctly. In case the promotion happens immediately after the current master becomes unavailable, some small network issue or delay can trigger the failover even when it is not necessary. In the other hand, the longer the waiting period is, the longer is also the resulting unavailability of the cluster. Also in case the communication quality between the nodes is not working well, it can happen that there are multiple nodes running in master mode at the same time. The accurately configured and well working failover is basically the only way how to achieve reasonable level of high availability in the single-master replication cluster.

\subsection{CAP theorem}
The CAP theorem \cite{cap}, also known as Brewer's theorem is a fundamental trade-off between concepts of \textit{consistency}, \textit{availability} and \textit{partition tolerance}. This idea was originally introduced by Eric Brewer in 2000 \cite{brewer_cap}.

The consistency condition states that all nodes of distributed system see the same data at the same time. In the world of databases it basically means, that read operation on any node including read-only replicas returns the value of the most recent write operation. A system is consistent if every transaction starts with the system in a consistent state and also ends with the consistent state.

Availability simply means, that read an write operations have to be successfully handled all the time. The system is not allowed to ignore client's requests.

The last requirement of the CAP theorem is the partition tolerance. Partition tolerance condition states that the system continue to work even in case some of the messages are delayed or lost on the way. The system can be partitioned into multiple groups, which can't communicate with each other. The system must be able to work correctly in such situation in order to be partition tolerant.

The core idea of CAP theorem is that it is theoretically not possible to offer all three conditions at the same time. The important point to think about, when designing a replicated system is which conditions are crucial in order to serve the purpose.

When designing the application supporting some business, calculating prices of the goods or services, the main priorities are the  consistency and partition tolerance. In case the application is for example collecting some data, the consistency is not critical, the data can be synchronized with some reasonable delay. The CAP theorem is visualized in Figure \ref{fig:cap}

\begin{figure}[H]
\caption{CAP theorem visualization}
\centering
\includegraphics[width=0.8\textwidth]{CAP}
\label{fig:cap}
\end{figure}

\section{PostgreSQL} \label{sec:postgresql}
The initial implementation of POSTGRES project, based on the relational data model started in 1986 at the University of California at Berkeley Computer, Science Department according to the PostgreSQL history \cite{pg_history} documentation page. The original query language PostQUEL was replaced by standardized SQL few years later, which resulted in choosing new name PostgreSQL for the whole project. PostgreSQL is an open source database, developed and supported by an active community. It runs on all major operating systems and it is extensible thanks to the concept of PostgreSQL extensions \cite{pg_extensions}.

PostgreSQL supports storing and querying the data in JavaScript Object Notation (JSON) format besides many traditional SQL data types. It is also not limited to the pure relational model design. The inheritance mechanism in PostgreSQL \cite{inheritance} allows to extend schema of already defined tables. The inheritance is a concept used mainly in Object-Oriented Database Management Systems \cite{oodbms}. The documentation defines PostgreSQL as an object-relational database, because of the support for mentioned features on top of the pure relational database principles.

\section{Replication in PostgreSQL} \label{sec:pg_replication}
PostgreSQL provides native support for replication setup to create an high availability cluster \cite{pg_ha}. PostgreSQL uses slightly different terminology and refers to master node as \textit{primary} replica and slave node as \textit{standby}.

Besides the native replication, there are multiple tools and systems for high availability. These tools provide additional features and require less amount of manual work and configuration. Following paragraphs introduce some of the available solutions for High availability for PostgreSQL.

\subsection{Native replication}
The native PostgreSQL replication is sufficient for building full, production ready replicated cluster.
It supports Warm Standby/Log Shipping high availability mode, which replicates the data to standby replicas, but they are not setup for handling queries. The standby replicas are only a backup, prepared to be promoted to master replica in case of a failover explained above. In version 9.0 the Hot Standby/Streaming Replication was added. The incoming read-only queries can be load balanced to standby replicas in the streaming replication mode and prevent overloading of the master node.

\subsection{Slony}
Slony-I \cite{slony} is an asynchronous replication system for PostgreSQL including the capabilities to replicate large databases. The design objectives were based on the analysis of existing replication systems at the time of the design. Some of the main goals were an extensibility of the replication system and the concept of cascading slave nodes. The idea of cascading is, that every node, which receives the can forward the update to other nodes. If slave nodes are able to send data to other slave replicas, it prevents overloading of the master node.

\subsection{Bucardo}
The use cases of Bucardo \cite{bucardo} replication program are similar to Slony-I. The core part of Bucardo is a daemon written in Perl \cite{perl} programming language.

The replication information needed for the daemon is stored in the main Bucardo database. This database contains list of databases involved in the replication, information about tables and replication mode. The daemon listens for the notify requests, connects to the remote databases and copies the data. The Bucardo daemon controlling the replication can be running on one of the servers involved in the replication or on the separate server. One of the advantages Bucardo provides is a multi-master replication mode.

\subsection{Repmgr} \label{sec:repmgr}
Repmgr \cite{repmgr} is an open source tool enhancing the PostgreSQL's built-in replication capabilities. Repmgr is a lightweight tool, it is implemented as a full PostgreSQL extension from version 4.0. It simplifies the cluster initialization and wraps parts of the replication setup and operations into one line commands.
The first important components of repmgr is a command-line tool serving to perform administrative actions like node setup, promoting of chosen slave to master and displaying cluster status.

Another component is a daemon called repmgrd \cite{repmgrd}. The administration can optionally run this management and monitoring daemon on each node in the replication cluster. Repmgrd is designed to automate many actions such as failover and pointing other slave to follow the new master node.

\subsection{Pglogical}
PostgreSQL version 10 introduced the initial support for logical replication, which is an alternative approach to the standard streaming (physical) replication in PostgreSQL. The difference between these two principles was explained in Section \ref{sec:types_of_replication}. The PostgreSQL extension called pglogical \cite{pglogical} was developed to utilize latest PostgreSQL features and supporting use cases like upgrades between major database versions, selective replication of sets of tables and data merge from multiple upstream servers.

\subsection{Pgpool-II} \label{sec:pgpool}
Even though the set of features provided by Pgpool-II \cite{pgpool} is overlapping with the previously explained tools, its design and purpose is different. Pgpool-II serves as a proxy, redirecting PostgreSQL database client requests to servers and resulting data back to the clients.

Connection pooling is the core feature of Pgpool-II. The established connections are reused more effectively thanks to this mechanism. Another useful feature is load balancing of read-only queries to multiple servers.

The replication itself and failover can be handled on Pgpool-II level as well. The data are not streamed from master node to other servers, but backed-up to multiple servers by Pgpool-II sitting in front of the database servers.

\chapter{Replicated PostgreSQL in OpenShift} \label{chap:pg_in_openshift}
Running containerized applications in Kubernetes and OpenShift (see sections \ref{sec:k8s} and \ref{sec:openshift}) is well established way of deployment and management of software in companies, which have already adapted Could Native approach. Relatively mature tools, workflows and well known best practices are available for stateless services, such as web applications, proxies and asynchronous workers managed by automated container management platform.

The data layer is an essential component of real world applications, especially in business sector. Data, stored in stateful database systems are critical parts of the system and cannot be doubled or replaced as easily as stateless components. Running database systems in containerized way can be very uneasy. The common practice is that applications running in the Kubernetes clusters alongside other stateless components are connecting to the external database system. This database system is usually replicated (see Section \ref{sec:db_replication}), but it is not containerized and managed by the container management platform.

Combining containerized application layer and standalone data layer is working sufficiently for many use cases. However the possibility to treat the data layer the same way as all the rest brings many benefits. The same tools can be used to monitor and manage all the layers, and the high availability and automation provided by Kubernetes can be minimize the database manual operational and maintenance work.

In the other hand some of the Kubernetes self healing capabilities bring higher probability of database restart or failover (see Section \ref{sec:failover}) even in case it is not necessary to keep the database cluster running. Running database clusters in Kubernetes is not widely adapted so far, there is a room for improvement and research.

\section{Objectives}
The first objective for the PostgreSQL Operator design is to explore existing solutions and tools on both sides, PostgreSQL replication and managing stateful applications in Kubernetes and Openshift. Tools having attributes, which satisfies the requirements of automated, containerized environment the best needs to be identified.

The following objective is to design and build container images containing selected tools for highly available PostgreSQL cluster and prepare them for running in the OpenShift environment.

The core objective is to design and implement PostgreSQL Operator itself as a Kubernetes operator, introduced in Section \ref{sec:operators}. This operator must be able to setup, monitor and manage replicated PostgreSQL cluster. It will be interacting with the previously created container images and its main responsibilities are to properly initialize multiple PostgreSQL replicas, keep them available and healthy and perform the failover (see Section \ref{sec:failover}) in the automated way, in case the master replica stops working correctly.

Preparation of the tests simulating crashes in the real production environment and testing the operator is the final step. Operators ability to initialize the cluster, scale number of replicas and handle the failover should be covered by tests.

\section{Selected technologies}
Operator Framework, described in Section \ref{sec:operator_framework} is basically the only adapted option to write Kubernetes operators for stateful applications. The framework provides three variant of the operator, pure Go operator, Ansible operator and Helm. Writing operator in Go programming language is the most mature option and it brings the best flexibility and customization in the operator design.

There is much more options and tools for the PostgreSQL replication implementation as it was described in Section \ref{sec:db_replication} focused on database replication types and Section \ref{sec:pg_replication} about various replication tools available for PostgreSQL.

The most important attributes for fully automated solution running in OpenShift or Kubernetes are easy to automate setup, possibility to scale number of replicas and ability to survive restart of any replica. The light weight solution are preferred, in order to keep size of the container images reasonably small. Master-slave replication is generally more standard and less complex approach, compared to multi-master model, also the selection of tools supporting master-slave is much wider.

The concept of logical replication (see Section \ref{sec:types_of_replication}) brings many benefits, like easier upgrade, better flexibility and replication of selected database subset. However, easy setup and fast scaling of the physical replication are critical for correctly working setup in OpenShift environment. The logical replication in PostgreSQL is also relatively new and it lacks working examples and documentation.

The asynchronous, physical replication based on master-slave model is the best match for automated solution, running in Linux containers ( see Section \ref{sec:containers}). When the advantage of light weight design and easy, setup and management in the automated way is taken into an account Repmgr \ref{sec:repmgr} is the most logical choice. Repmgr is implemented as PostgreSQL extension installed alongside standard PostgreSQL server. It is easy to setup and automates cluster operations including failover thanks to repmgrd.

For connection pooling and load balancing of read-only requests in PostgreSQL, Pgpool-II (see Section \ref{sec:pgpool} is the only mature technology available.

\section{Cluster design}
The high level design of the cluster implementing high availability for PostgreSQL in OpenShift environment is visualized in Figure \ref{fig:cluster_design}.

\begin{figure}[H]
\caption{Design of PostgreSQL cluster in OpenShift}
\centering
\includegraphics[width=1\textwidth]{cluster-design}
\label{fig:cluster_design}
\end{figure}

There is N containers containing PostgreSQL and Repmgr extension. One container is running as a master node and the rest of them are slaves. Slave nodes are running in \textit{hot standby} mode mentioned in Section \ref{sec:pg_replication}, dedicated to replication options provided by PostgreSQL.

Each node will be backed by persistent storage. The storage is approached in Kubernetes in very abstract way as it was described in Section \ref{sec:pv}. The PostgreSQL cluster implementation shouldn't limit this abstract approach in any way. The administrator is able to plug in the storage of his choice.

One Kubernetes service (see Section \ref{sec:service}) serves as a proxy for current master. This primary service provides stable domain name for write queries. In case the master is restarted or even when the failover occurs and IP address of the master container can change, service stays in place and it is redirecting incoming requests to healthy pods.

The second service is load balancing incoming requests to all the PostgreSQL nodes in the cluster. Slaves are running in hot standby mode, so this service can be used for read-only query to prevent overloading of master replica.

Pgpool-II containers are used as a proxy. Pgpool-II replication and failover capabilities are disabled, this is handled on Repmgr level. Pgpool-II proxies are configured to load balance read-only queries to  read-only service. Pgpool-II load balancing is session based, which means that consequent requests from single client goes to the same database node. This approach minimizes the potential issues related to asynchronous replication.

\section{PostgreSQL container images}
The container images for replicated cluster need to contain PostgreSQL server, tooling and repmgr extension to deliver high availability. The second requirement is to satisfy all the OpenShift security policies and be able to run in the cloud environment without big amount of manual configuration required.

Building similar image from scratch would require a lot of work and effort. One of the design goals of container images, introduced in Section \ref{sec:containers} is to make existing artifacts extensible.

Number of database nodes, Pgpool-II replicas and many other configuration details of the cluster can be adjusted by the administrator using cluster API. 

PostgreSQL container images \cite{pg_cnt} are an open source collection of container images designed to run in OpenShift. This repository supports multiple stable versions of PostgreSQL and each of them based on RHEL \cite{rhel}, CentOS \cite{centos} and Fedora \cite{getfedora} operating systems.

Both RHEL and CentOS are the long term support distributions, they guarantee the API stability for long time period and there is not an easy way to add new packages or features to them in a reasonable time frame. Fedora in the other hand is a community distribution opened to the latest software of different kind, having a new release twice a year.

Fedora based image from PostgreSQL container images project is available with complete installation of PostgreSQL version 10 and it is running in OpenShift very well. The only missing piece is the repmgr extension.

Repmgr was not available in the standard RPM \cite{rpm} format, supported by Fedora distribution. It was necessary to build, test and create a new Fedora package for repmgr \cite{fedora_repmgr} as part of implementing high-availability for PostgreSQL in OpenShift. The guidelines for making new RPM packages \cite{fedora_new_package} were followed.

After the package was approved and added to Fedora repositories \cite{fedora_repositories} it was necessary to add this package to existing Fedora based PostgreSQL 10 container image \cite{pg_img}.

PostgreSQL container image also provides a mechanism for image extending \cite{pg_cnt_extending}. Custom initialization and configuration scripts, enabling the repmgr extension can be added to the image using this feature.

\section{PostgreSQL Operator}
PostgreSQL Operator is based on the latest template for Go operators generated by Operator SDK, explained in Operator Framework Section \ref{sec:operator_framework}. The core parts of Operator design are the API, explained in more details in the following subsection, internal representation of the PostgreSQL nodes running in the cluster, control loop logic and handlers for managing underlying fundamental Kubernetes objects.

When the reconcile loop is triggered, the operator control logic iterates over map of all desired nodes, defined via operator API and ensures the current nodes running in the cluster reflects the desired state. In case there are some differences, nodes in the cluster are created, removed or modified according to the current specification.

\subsection{Operator API} \label{sec:operator_api}
The Operator Framework provides a mechanism to create a Custom Resource definition. Custom Resource has its own API in form of the Kubernetes manifest (see Section \ref{sec:manifest}) in YAML format. This manifest contains the usual \textit{apiVersion}, \textit{metadata} and \textit{spec} fields. The structure, semantics and allowed values under spec field are defined by the Custom Resource author. The manifests content is parsed by the operator each time the reconciliation loop is triggered and the latest specification from the manifest is available to the operator logic.

The first attempt to define an API for PostgreSQL has a simplistic design.
It is providing common node specification for whole cluster:

\begin{lstlisting}
apiVersion: postgresql.openshift.io/v1
kind: PostgreSQL
metadata:
  name: postgresql-cluster
spec:
  managementState: managed
  size: 3
  nodeSpec:
    image: "mcyprian/postgresql-10-fedora29:1.2"
    resources:
      limits:
        memory: 1Gi
        cpu: "750m"
      requests:
        memory: 512Mi
        cpu: "250m"
    storage:
      emptyDir: {}
\end{lstlisting}

The \textit{managementState} field simply defines, whether the operator is active or not. There can be some situation, when administrator needs to perform some operation on the cluster and don't want the Operator to touch the cluster or modify its state. When the management state is set to \textit{unmanaged} the Operators control loop is an empty operation.

Size of the cluster specifies how many nodes should run in the cluster. As it is an implementation of master-slave replication \textit{size: 3} means, there will be a single master and two slave replicas.

Image field serves for specifying the container image used for the nodes. Minor version update can be performed by changing the image tag to higher number.

The \textit{resources} field reflects the standard way of deployment resources definition in Kubernetes as explained in Section \ref{sec:resources}.

Similarly to resources, the \textit{storage} field can contain the storage provisioner of different kinds and settings specific to selected provisioner. The storage abstraction in Kubernetes was explained in Persistent volumes Section \ref{sec:pv}.

This initial API design is simple and straightforward, a replicated PostgreSQL cluster of desired size can be defined on few lines. However, when considering practical scenarios of the cluster administration, this design doesn't provide much flexibility. In case the administrator plans a minor update of the cluster, he has to update all the nodes, including master node at once. It also doesn't allow to identify the slave nodes and try some different experimental kind of storage without affecting master node.

The final operator API design is slightly more complex and gives the administrator more flexibility in the cluster definition. Each node is identified by its name and separate configuration for each of them can be defined:

\begin{lstlisting}
apiVersion: postgresql.openshift.io/v1
kind: PostgreSQL
metadata:
  name: postgresql-cluster
spec:
  managementState: managed
  nodes:
    node-one:
      image: "mcyprian/postgresql-10-fedora29:1.2"
      priority: 100
      resources:
        limits:
          memory: 1Gi
          cpu: "750m"
        requests:
          memory: 512Mi
          cpu: "250m"
      storage:
        storageClassName: local-storage
        size: 20Gi
    node-two:
      image: "mcyprian/postgresql-10-fedora29:1.2"
      priority: 60
      resources:
        limits:
          memory: 1Gi
          cpu: "750m"
        requests:
          memory: 512Mi
          cpu: "250m"
      storage:
        storageClassName: local-storage
        size: 20Gi
    node-experimental:
      image: "mcyprian/postgresql-10-fedora29:latest"
      priority: 0
      storage:
        storageClassName: local-storage
        size: 20Gi
\end{lstlisting}

This solution defines nodes as a key value map. Each node can have specific resource requests and limits, it can be using different storage provisioner and storage settings and can use different container image. Additional \textit{priority} field was added to define node priority to became master node during the cluster initialization after the failover (see Section \ref{sec:failover}). In case the administrator wants to experiment with some of the slave nodes, he can set its priority to zero and make sure this node won't be promoted to master before the experiment is finished.

\subsection{Internal representation of nodes}
 Each node defined in the operator API nodes map is represented as a Go structure called \textit{deploymentNode} in the operator logic. The operator creates a Kubernetes deployment (see Section \ref{sec:deployment}) specifying a single replica pod. Cluster name and namespace are propagated into the deployment definition. There is also one Kubernetes service created for each node to have a stable domain name to access node inside the cluster.

 Go structure contains references to the deployment and service structures of the node and connection to its database. The database connection serves mainly to retrieve current replication status from inside the pod. Alongside the current status deploymentNode structure provides methods to create, update and delete the node.

 The operator control loop works with the map of deploymentNode structures and executes methods this structure provides to make cluster reflect the desired state.

\section{Cluster initialization}
\section{Database interaction}
Operator Framework (see Section \ref{sec:operator_framework}) allows using different Go libraries for interaction with the Kubernetes cluster. The most common way is using Kubernetes API \cite{strong-api}. This robust API provides calls to read an modify all kinds of Kubernetes objects in the cluster. Kubernetes API can be used to get pod, its containers specification and metadata and also to modify container resources, startup command, environment variables and many other attributes. However, API can't access the content of containers running in Kubernetes cluster. The usual way to read or debug filesystem inside container is executing shell inside running container.

Repmgr introduced in Section \ref{sec:repmgr} stores the cluster information in the PostgreSQL database system itself. Each node contains replication database repmgr containing important data about all the nodes, its roles and accessibility. The \textit{repmgr.nodes} table for simple replicated cluster with one master node and three slaves looks similar to example in Table \ref{table:repmgr_nodes}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c c c c c|}
 \hline
 node\_id & active & node\_name & type & priority \\ [0.5ex]
 \hline
 1 & t & node-one & primary & 100 \\

 2 & t & node-two & standby & 60 \\

 3 & t & node-three & standby & 30 \\

 4 & t & node-extra & standby & 0 \\ [1ex]
 \hline
\end{tabular}
\caption{Example of cluster representation in \textit{repmgr.nodes} table.}
\label{table:repmgr_nodes}
\end{table}

Data stored in \textit{repmgr.nodes} tables on replicated nodes are critical for correct behavior and setting of the cluster proxies and detection of the node misbehavior. Also, these data needs to be exposed to the administrator in operator status. Executing shell inside each node during every reconcile loop to get recent updates from inside the cluster is not very clean design. It would be necessary to parse data returned from exec command in text format and hard to detect errors.

Alternative solution for interacting with the database from operator is to use database connections. Go provides package for handling PostgreSQL connections, queries and store results into native structures of the language. Constructor of the deploymentNode structure, explained in previous section opens the connection to PostgreSQL server running inside container using repmgr user and database. This database connection is stored alongside node's Kubernetes objects in the deploymentNode structure. Database queries are an efficient way to get fresh information about the replicated cluster nodes into the operator logic.

\section{Password management}

\section{Failover implementation}
The failover principle was described in Section \ref{sec:failover}. When some  kind of serious error occurs on the master node in single-master replication, one of the slave replicas is promoted to new master.

\chapter{Testing} \label{chap:testing}

\chapter{Conclusion} \label{chap:conclusion}

\appendix %% Start the appendices.
\chapter{List of electronic appendices}
Electronic appendices contain source codes, container image definitions and other software components implemented as part of the highly available solution for PostgreSQL running in OpenShift:
\begin{itemize}
  \item postgresql-operator - source code of the PostgreSQL operator
  \item postgresql-container - Fedora based PostgreSQL container image, containing Repmgr extension and shell scripts
  \item pgpool-fedora-container - Fedora based Pgpool-II container image and corresponding Kubernetes manifests
  \item repmgr.spec - spec file of repmgr RPM package
\end{itemize}

\end{document}
