%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  twoside, %% This option enables double-sided typesetting. Use at
           %% least 120 g/m² paper to prevent show-through. Replace
           %% with `oneside` to use one-sided typesetting; use only
           %% if you don’t have access to a double-sided printer,
           %% or if one-sided typesetting is a formal requirement
           %% at your faculty.
  table,   %% This option causes the coloring of tables. Replace
           %% with `notable` to restore plain LaTeX tables.
  lof,     %% This option prints the List of Figures. Replace with
           %% `nolof` to hide the List of Figures.
  lot,     %% This option prints the List of Tables. Replace with
           %% `nolot` to hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = \the\year/\the\month/\the\day,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Michal Cyprian,
    gender        = m,
    advisor       = doc. RNDr. Vlastislav Dohnal Ph.D.,
    title         = {High-availability for PostgreSQL in OpenShift},
    keywords      = {PostgreSQL, OpenShift, Kubernetes, High-availability, Relational Databases, Data Replication, Operator Framework},
    abstract      = {%
      Running containerized applications in Kubernetes and OpenShift is a well-established way of deployment and management of software in companies, which have already adopted Could Native approach to the software life cycle. Relatively mature tools, workflows and well-known best practices are available for stateless services, such as web applications and proxies managed by automated container management platform. It is not very common to deploy stateful systems as databases into the same environments. This thesis focuses on the problematic of integrating stateful, highly available database cluster into OpenShift. Existing tools are surveyed, analyzed for their deployment in automated cloud environments and benefits as well as threats. The solution consists of container images containing selected PostgreSQL tools for high-availability and operator managing these containers in the OpenShift environment. The operator can initialize the cluster, detect crashes and autonomously perform failover.
    },
    thanks        = {%
      Firstly, I would like to thank my thesis advisor doc. RNDr. Vlastislav Dohnal, Ph.D. for his support, patience and many pieces of valuable advice. I am also grateful to the people working on the OpenShift components, especially Jan Horák, who shared his technical knowledge and experience with me and was always willing to discuss my ideas and problems, that appeared during my research and design of the solution.
    },
    bib           = citations.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[hyphens]{url} %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{tabularx} %% Tables
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{listings} %% Source code highlighting
\usepackage{graphicx} %% image management
\graphicspath{ {fithesis/images/} }
\lstset{
  basicstyle        = \footnotesize,
  identifierstyle   = \color{black},
  numbers           = left,
  stepnumber        = 1,
  numbersep         = 10pt,
  tabsize           = 1,
  keywordstyle      = \color{blue},
  keywordstyle      = {[2]\color{cyan}},
  keywordstyle      = {[3]\color{olive}},
  stringstyle       = \color{teal},
  commentstyle      = \itshape\color{magenta},
  breaklines        = true,
  breakatwhitespace = false,
  showstringspaces  = false,
  frame             = single,
}

\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
%% The following code fixes the rendering of BibLaTeX ISO 690
%% references in old TeX Live (such as the one at Overleaf).
\thesisload
\makeatletter
\def\thesis@biblatexiso@fix@package{iso-numeric.bbx}
\def\thesis@biblatexiso@fix@end{\relax}
\newif\ifthesis@biblatexiso@fix@
\thesis@biblatexiso@fix@false
\def\thesis@biblatexiso@fix@next#1,{%
  \def\thesis@biblatexiso@fix@current{#1}%
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@package
    \thesis@biblatexiso@fix@true
  \fi
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@end
    \expandafter
    \@gobble
  \fi
  \thesis@biblatexiso@fix@next
}
\expandafter\expandafter\expandafter\thesis@biblatexiso@fix@next\@filelist,\relax,
\ifthesis@biblatexiso@fix@
  \defbibenvironment{bibliography}
    {\list%
       {\MethodFormat}%
       {\setlength{\labelwidth}{\labelnumberwidth}%
        \setlength{\leftmargin}{\labelwidth}%
        \setlength{\labelsep}{\biblabelsep}%
        \addtolength{\leftmargin}{\labelsep}%
        \setlength{\itemsep}{\bibitemsep}%
        \setlength{\parsep}{\bibparsep}}%
        \renewcommand*{\makelabel}[1]{\hss##1}
        }%
    {\endlist}%
  {\item}%
\fi
\makeatother
\begin{document}
\chapter{Introduction}
We live in the computer age. People can access information and knowledge faster and easier than ever before. The evolution of information technology in the last few decades had a significant impact on our society. The bulky personal computers have been replaced by tiny devices, laptops, cell phones and smartwatches. The performance has been increased, internet connection is available almost everywhere, and the number of easy to use applications and web pages for various purposes has grown significantly.

This revolution also brought a lots of new challenges and rapid changes on the other side of computer industry. All the data centers, where pieces of information are stored, computers, which runs those services, web pages and applications must have been adapted to the requirements of this age equally well.

The main goals of web service providers are to make the service highly available, capable of handling a big number of requests each second and update running services smoothly. The service should survive disk corruption on some of the computers it is running on, small network issues and sometimes event natural disasters in the area, where the data center is located. Ideally, the user shouldn't experience any downtime of the service, when it is being updated to a newer version. Software engineers have started an effort to develop complex platforms able to face such challenges in an automated way.

Automated platforms currently work quite well for a simple application. An additional level of complexity is added when systems storing a specific state, such as databases are managed automatically by the platform. This area is still relatively unexplored and provides plenty of fascinating design problems to be solved. That is the main reason why I have chosen integrating highly available database system into the automated platform as a topic for my thesis.

The main goal of this thesis is to explore existing tools and solutions for both, database high availability and the automated platforms, choose the most suitable tools to be integrated into a single system and implement the missing automation logic. The resulting solution should be able to handle real-world use cases of highly available database system autonomously. The final part of the thesis is to prepare different test scenarios, including misbehaviour and crashes and evaluate the behaviour of the system.

The following Chapter \ref{chap:cloud_native} describes the recent transformation of the approach to design and manage applications. The importance and core principles of automated platforms for service management, which are necessary for a deeper understanding of the thesis topic are explained.

Chapter \ref{chap:ha_database_systems} provides theoretical background and summarizes the current state of the database systems, especially solutions to achieve high availability in database systems, which are the second important part of the topic.

The next Chapter \ref{chap:pg_in_openshift} focuses on the design, implementation and testing of the high-availability solution for PostgreSQL database system running on OpenShift platform, based on the research and theoretical knowledge covered in two previous chapters.

The process of automated and manual testing of the solution is covered in Chapter \ref{chap:testing}. Crashes of database system nodes are simulated and the performance and availability of the system in different scenarios is measured. This Chapter summarizes the process of testing and all the results. Test results are followed by the final conclusion (Chapter \ref{chap:conclusion}).

\chapter{Cloud native computing} \label{chap:cloud_native}
The new approach to design, create and deploy applications is much different from the traditional way, which includes many manual stages. The main goals are to get new services to the market faster, minimize the cost and increase the scalability of the services. Automation of all the service life cycle stages is the key to achieve these goals. Principles like microservices architecture, continuous delivery and container orchestration platforms are used for automation of software life cycle processes. This particular approach to application life cycle combined with new tooling is usually called "Cloud Native" \cite{cloud_native}.

The following chapter describes the core components of concrete container orchestration platforms and related Cloud Native technologies.

\section{Linux containers} \label{sec:containers}
The idea of isolating some processes from the rest of the operating system has a long history. There are many reasons for isolating part of the system. Sandboxing applications, which can't be trusted can prevent security threats. Installing application dependencies into a separate file system can help to avoid requirement version conflicts and isolated program alongside its dependencies can be distributed to different machines with a lower risk of introducing issues.

The invention of the virtual machine is connected with IBM and its effort to share usage of computer resources among groups of users, which started in 1960s \cite{vm_history}. The virtualization concept has evolved into a complex technology. The core principle is in running multiple operating systems on a single physical machine. This is usually achieved by making an abstraction of the physical hardware. Virtualization is used heavily in the data centres and also on desktop computers. However, running many operating systems is not ideal for some use cases and there were multiple attempts to develop isolation on the operating system level.

The concept of chroot \cite{chroot} was introduced during the development of Unix Version 7 in 1979. The chroot system call serves to isolate the process by changing the root directory of the new process to a different path in the system. The process isolated by chroot and its children can only access the directories within its own directory tree. This concept is considered to be the first implementation of lightweight isolation. The chroot system call has still its use cases, forty years after its original implementation.

The concept of chroot improved in several ways later served as a base for FreeBSD Jails \cite{freebsd_jails}. The separation implemented by jails is not limited to the file system access. It allows isolating all the system resources like users, processes and networking subsystem. The administrator can divide the system into several independent units. Each jail can have an IP address and its own configuration.

In 2006 Google engineers launched a project called Process Containers. This feature allows partitioning of resources such as CPU time, system memory, disk and network bandwidth into groups and assigning tasks to these groups. The resource limits for a collection of processes can be specified this way. This feature was later renamed to Control Groups (cgroups) and merged into Linux kernel 2.6.24 \cite{cgroups}.

Another important element in Linux process isolation are the Linux Namespaces \cite{linux_namespaces}. Namespaces provide processes with their own view of the system resources. There are seven kinds of namespaces managing the visibility of different resources:
\begin{itemize}
  \item Mount - serves for isolation of mount points, its similar to chroot system call but provides complete isolation
  \item Process ID - the processes within a specific PID namespace can only see processes in the same namespace, multiple nested process trees can exist on the system
  \item User - isolates UID/GID number spaces
  \item Network - allows creating a virtual network stack for each process
  \item UTS - namespace allowing each process to have a specific hostname and domain name
  \item IPC - separates interprocess communication resources
  \item Cgroup - isolates cgroup root directory
\end{itemize}


Control Groups alongside namespaces are fundamental aspects of a operating system level virtualization on Linux. Isolated virtual instances using a single shared Linux kernel are called containers. The container image is a term used for a static file, which includes guest operating system components, system libraries and the executable application itself. The name container refers to an instance of container image running as an isolated process.

LXC \cite{lxc}, containerd \cite{containerd}, rkt \cite{rkt}, CRI-O \cite{cri-o} and many others are the implementations of Linux container runtime, which have been introduced since 2008. All of these implementations are based on similar principles. Not only container runtimes but also other kinds of tools for building, managing and orchestrating containers have been developed since the initial rise of containers. After the initial period, when the technology was relatively unstable and challenging, it finally becomes mature enough to be widely used in production.

When comparing Linux containers to the concept of virtual machines, the traditional virtualization runs an entire guest operating system in a virtual machine while containers share the kernel of the host system. Figure \ref{fig:cnt-vs-vm} shows the difference in virtualization layers between Virtual Machine type 1 hypervisor \cite{hypervisors} and Linux containers. Thanks to its lightweight design, containerization provides shorter build and startup times, smaller image size and real-time provisioning and scalability. On the other hand, process level isolation is less secure than full isolation of the guests systems. It is necessary to follow the best practices when working with Linux containers in order to meet the security standards for production systems. In general Linux containers satisfy the requirements of Cloud Native approach workflows better than heavyweight virtualization methods.

\begin{figure}[H]
\caption{Comparison of virtual machines and containers}
\centering
\includegraphics[width=1\textwidth]{cnt-vs-vm}
\label{fig:cnt-vs-vm}
\end{figure}

\section{Container orchestration} \label{sec:orchestration}
Containers as a standardized format carrying everything the application needs to run is an efficient fundamental element for building scalable services. However, this is only the first step on the long way to solving the problem of running a large number of services at scale. The cluster is often composed of many application instances to handle thousands of request each minute and survive misbehaving of failure of some of the containers. The service itself is usually divided into multiple units, communicating with each other. The application needs to be backed by some kind of data store or database and it can also use caches.

The management and coordination of such cluster include many different activities. All these containers must be scheduled and started at the right moment, sometimes they must start in a particular order. In case some of the containers fail, another container of the same kind must be scheduled to replace it. The application endpoints must be exposed to the outside world. In case the new version of an application is being deployed, the old containers must be replaced in the smart way to minimize the downtime of the application. The term container orchestration in used for coordinating and sequencing this kind of activities in the cluster.

\section{Kubernetes} \label{sec:k8s}
Many companies started inventing container orchestration systems, the platforms to schedule, manage and run containers. Need for similar platforms is connected with the rise of the microservice architecture of the applications. Borg \cite{borg} is one of such systems developed at Google to run their services in containers. The new project named Kubernetes was started by Google engineers in 2014. The goal was to create an open source container orchestration, which can be adopted by anyone based on the experience gained on Borg.

The quick adaption and success of Kubernetes are described with the following words in the book Cloud Native DevOps with Kubernetes:
\begin{quote}
Kubernetes’s rise was meteoric. While other container orchestration systems existed
before Kubernetes, they were commercial products tied to a vendor, and that was
always a barrier to their widespread adoption. With the advent of a truly free and
open source container orchestration, adoption of both containers and Kubernetes grew
at a phenomenal rate.
By late 2017, the orchestration wars were over, and Kubernetes had won. While other
systems are still in use, from now on companies looking to move their infrastructure
to containers only need to target one platform: Kubernetes. \cite[p. 11]{k8s}.
\end{quote}

The key benefits of Kubernetes \cite{k8s_features} besides automation of operational workflows are the high-availability, load balancing, auto-scaling, self-monitoring and self-healing. The high-availability is achieved thanks to distributing the important parts of the system over multiple nodes (see Section \ref{sec:node}). Load balancing together with auto-scaling prevents overloading of services running in Kubernetes. Incoming requests are distributed across a set of containers and in case the set is not big enough according to some monitored metrics, more containers are added automatically. Containers running in the Kubernetes cluster are monitored and replaced or restarted in case some failures are detected, which makes the platform self-healing.

Following subsections describe the fundamental principles of Kubernetes. This container management system is designed to solve different problems, using plenty of highly customizable objects and it is still evolving quite quickly. It is not possible to fully explain the complex structure of Kubernetes in the scope of one section, but it covers the basic principles necessary to understand the design of the solution explained in the following chapters. The base objects described in this section are visualized in Figure \ref{fig:k8s_objects}.


\begin{figure}[H]
\caption{Visualization of basic Kubernetes objects}
\centering
\includegraphics[width=1\textwidth]{k8s-objects}
\label{fig:k8s_objects}
\end{figure}

\subsection{Deployment} \label{sec:deployment}
Kubernetes uses many abstractions to be adaptable for different use cases. It is not enough to schedule and runs the container. The containerized application can stop working properly if there is a memory error, disk corruption or some other issue. In case the container fails, it must be replaced by another one with the same image, command and configuration. The supervisor, which stores the container specification and is used for periodic checks if all the desired containers are running and responding is called the \textit{deployment}.

\subsection{Pod} \label{sec:pod}
A \textit{pod} is the second fundamental Kubernetes object. It represents a group of containers, which are scheduled together. They also usually share storage and the IP address. In most cases, there is a single container per pod. The deployments define the desired state of pods, when the deployment is created and a pod satisfying the deployment doesn't exist, it is scheduled and created. The pod is the smallest deployable unit in Kubernetes.

\subsection{Node} \label{sec:node}
Pods encapsulate containers, which are scheduled and run. The host operating system, with container runtime installed, is needed for running containers as explained in Section \ref{sec:containers}. This is the role of \textit{nodes}. A node can be virtual or physical machine containing software necessary to run pods. Distributing pods over multiple nodes is one of the main pillars of high availability in Kubernetes.

If there are no special constraints configured, the Kubernetes scheduler \cite{kube-scheduler} assigns new pod to one of the nodes, based on multiple criteria. The scheduler logic takes into account if candidate node has sufficient resources to run a pod and tries to find a node with the highest score based on its local container image cache, resource balance and many other factors.

\subsection{Service} \label{sec:service}
Pods are volatile and often short-lived. A pod can crash, it can be restarted or it can be removed by the pod autoscaler \cite{hpa} at some point. This is problematic, because the network connections to the pod can't rely on its IP address, which can change at any moment. Usually, there are multiple replicas of the pod with different addresses. The concept of Kubernetes \textit{service} provides the single, consistent IP address and load balances the incoming requests to a set of pods.

\subsection{Persistent volumes} \label{sec:pv}
Each pod has its own file system, but it is ephemeral. At the moment when the pod is restarted all the data are lost. This storage is sufficient for the configuration, which is regenerated during the initialization of the pod, but not for data that must survive a restart of the pod. Also in case, there are multiple containers running in a pod, they might need to share some files.

The \textit{PersistentVolume} (PV) object solves both of these problems, file sharing and storing the data persistently. It is mounted to the pod and it is accessible for the processes running inside its containers. The PV is just an abstraction, plenty of different underlying storage types are supported. A \textit{PersistentVolumeClaim} (PVC) is a request for storage, that can be consumed by its user. The PVC defines the size of the storage and access mode. If the volumes needs to be provisioned dynamically, \textit{StorageClass} \cite{storage-classes} object can be used to specify the provisioner.

\subsection{Resources} \label{sec:resources}
Each pod definition in Kubernetes can contain a resource requests and limit for its containers \cite{mcrc}. The two most frequently used types of resources are \textit{CPU} specified in core units and \textit{memory} specified in bytes.

Specified resource requests of the container are used by Kubernetes scheduler to select the node for a new pod. In case the container exceed one of the resource limits it can be restarted or terminated.

\subsection{Object organization} \label{sec:obj_organization}
There is usually plenty of various objects running inside a Kubernetes cluster. It is often necessary to divide them into groups and subsets. Kubernetes supports \textit{namespaces} \cite{namespaces}, virtual isolated clusters built on top of single physical cluster. Namespaces serve to divide objects owned by different teams or deployments of different projects.

Objects living within specific namespace also needs to be organized and divided into smaller sets. Concept of Kubernetes \textit{labels} and \textit{selectors} \cite{labels_selectors} are designed for this purpose.

A label is a key/value pair, which can be attached to objects. Each object, for example, deployment, pod or service can have multiple labels. A label is not unique, all the objects belonging into a certain group have exactly the same key and value attached. An example of a Kubernetes manifest in the following section shows, how labels can be attached to the deployment and all pods satisfying that deployment.

Label selectors are used for identifying all objects sharing a specific label. A good example is a Kubernetes service, explained above, which provides a consistent IP address for a set of pods. Each service has a label selector defined and it is load balancing incoming requests to all pods carrying the label from this selector.

\subsection{Manifests} \label{sec:manifest}
Objects, like pods, deployments and services are usually defined, configured and managed by the administrator directly of by automated scripts. The most frequent way of Kubernetes objects definition is to write a description of the object into file following YAML \cite{YAML} standard. Descriptions of Kubernetes objects are called manifests. Kubernetes manifests can be sent to the Kubernetes API server \cite{kubernetes-api} using \textit{kubectl} \cite{kubectl} command-line tool and are used to create, modify and delete objects in the cluster.

Following example shows a manifest containing a definition of a simple Deployment object in YAML format:
\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
\end{lstlisting}

This manifest defines the deployment of three desired replicas of Nginx \cite{nginx} container, which expose port 80. The deployment and all the replicas are labelled for better organization of objects in the cluster.

\section{OpenShift} \label{sec:openshift}
OpenShift \cite{rhccos} is a Kubernetes container platform product having a paid support. Openshift uses Kubernetes as a base and builds additional layers of policies, container image streams and supporting software components on top of it. OpenShift brings stricter security policies, integrated CI/CD and web console for cluster management. From version OpenShift 4 there is also an integrated OperatorHub \cite{operatorhub}, storing community operators. These operators become one of the common ways to deploy services on OpenShift and Kubernetes.

Using OpenShift instead of pure open source Kubernetes project components brings also some limitations. Kubernetes supports many Linux distribution, while Openshift can be easily installed only on distributions like Red Hat Enterprise Linux (RHEL) \cite{rhel}, CentOS \cite{centos} and a couple more Red Hat alternatives.

\section{Minikube} \label{sec:minikube}
The distributed model is the core feature of Kubernetes, all the resource and important data about the desired cluster state are spread across multiple physical or virtual machines. The setup of fully working distributed Kubernetes cluster requires a lot of resources, configuration and maintenance work.

When the Kubernetes applications are being developed locally or need to be tested, building highly available distributed cluster is usually not necessary. Minikube \cite{minikube} is a tool for running single-node Kubernetes cluster locally. It supports all the major Kubernetes features and it can run on average personal computer or laptop.

There is also an option to run the OpenShift cluster locally. Minishift \cite{minishift} is designed similarly to Minikube, it is a lightweight solution that allows running single node cluster containing also all the OpenShift layers on top of Kubernetes.

\section{Operators} \label{sec:operators}
The main goal of Kubernetes design is automation. The fundamental Kubernetes objects, such as deployments are periodically checking the number of existing replicas, their health, status and resource usage. In case some issues are detected in the cluster, various operation actions are executed. This mechanism can automatically handle common misbehaving like random process failures or overloading of replicas.

If the Kubernetes setup for stateless web application is done properly, the basic objects are usually enough to create robust and stable service, which can work with a minimal number of human operational actions. The world of stateful applications brings much more challenges. Working with a database cluster usually requires many manual operations.

\subsection{Kubernetes API extensions}
The concept of operators \cite{kubernetes-operator} was introduced by CoreOS \cite{operators} in 2016 aiming to solve the problem of hard integration of stateless applications into Kubernetes. The operators are the Kubernetes extensions, which serve to include operational domain knowledge into management of a complex application. The main idea of operators is to extend Kubernetes API without modifying code of the container orchestration system itself and create a new kind of object. This object has the API similar to basic Kubernetes objects but operates on a higher level of abstraction.

In case there are some repetitive patterns in the manual operational actions, it means the workflow can be at least partially automated using operator. An operator can, for example, provide a database cluster, encapsulating the logic of deployment and management by combining many fundamental Kubernetes objects. The operator is periodically checking the state of the cluster in a loop and can react to specific events in the cluster by modifying underlying objects. The common examples of Kubernetes operators are implementations of cluster logging and monitoring or management of database clusters of different types.

\subsection{Operator Framework} \label{sec:operator_framework}
The Operator Framework \cite{operator-framework} was released two years after the introduction of operators. This framework provides an SDK, which allows building operators without deep knowledge of low-level Kubernetes API, and a couple of other tools supporting the development of operators. The Operator SDK serves for faster setup of operator project and generating the boilerplate code. It provides libraries for operator API definition, implementation of the control loop and a framework for end-to-end testing of operators.

The API can be extended by creating a Custom Resource \cite{custom-resources}, supported by Kubernetes. A new Custom Resource is defined by operator author and it exposes an API in form of the custom manifest \ref{sec:manifest}. This manifest can be used by the administrator to create a Custom Resource in the cluster and modify its configuration after the operator is deployed to the cluster. It is similar to the way deployments (see Section \ref{sec:deployment}) and other Kubernetes objects are managed. Currently applied manifests are periodically compared to the state of objects in the cluster and an operator performs implemented operations based on the Custom Resource configuration. The operator itself is a pod running in a cluster, holding the logic to manage the Custom Resource, monitoring other objects in the cluster and performing programmed operational actions.

The Operator SDK currently supports three types of operators \cite{operatorhub-sdk}. The first option is building the operator as a pure program in Go programming language \cite{golang}. The SDK generates the boilerplate code, an example YAML manifest introduced in Section \ref{sec:manifest} and the endpoint for reconciliation loop. The second option is to create an Ansible operator. Ansible \cite{ansible} is a software provisioning and application deployment tool. Operator SDK allows to define an operator in the form of Ansible playbook \cite{wwp} and the SDK generates the Go program, which runs the playbook to manage Kubernetes objects. The last supported variant is a Helm operator. Helm \cite{helm} is a package manager for Kubernetes, which supports templates for manifests, creating reproducible builds and installing and managing Kubernetes applications.

\chapter{Highly available database systems} \label{chap:ha_database_systems}
The problems of data redundancy, consistency and atomicity of operations led to an effort to develop dedicated systems for storing and managing the data. Various types of database systems have been evolved over time to satisfy different technical and business use cases. The data loss affects the business significantly in most of the cases. The great importance of the data leads to necessity of implementing high availability for the database systems. Achieving high availability for database systems is more complex compared to stateless systems, because there must be a single source of truth condition satisfied at the same time.

Following sections introduce concept of relational model, database systems based on this model, and a theoretical background for database systems high availability. The second part of this chapter focuses on PostgreSQL and various tools designed for building highly available clusters using this database system.

\section{Relational database systems}
The term relational database was used for the first time by E. F. Codd in his paper \cite{codd_relational_model} titled \textit{A Relational Model of Data for Large Shared Data Banks} from 1970. A new method of data storing and processing is described in this paper. The modern relational databases are still based on similar principles. Relational databases are based on the relational model, which can be explained by following sentences:
\begin{quote}
The relational model uses a collection of tables to represent both, data and the relationships among those data. Each table has multiple columns, and each column has a unique name. Tables are also known as relations. \cite[p. 9]{db}.
\end{quote}

The relational database systems using standardized Structured Query Language (SQL) \cite{sql_standard} become the most commonly used database solutions for web based applications in last decades.

\section{Database replication} \label{sec:db_replication}
When the relational databases are used to store important data, supporting some critical business or research, the database system must be highly available and immune to overloading. These requirements are similar to the key features of Kubernetes explained in the previous chapter. The solutions bringing these capabilities to database systems are also based mainly on building a multi node database cluster.

In case the database system is composed of multiple nodes (servers), the data must by synchronized between all the nodes. The process of permanent copying of data changes from one node to another is called database replication.

\subsection{Types of replication} \label{sec:types_of_replication}
There are multiple approaches to database replication, each of them has some advantages and downsides as well. There are also various criteria, which can be used to categorize replication types. The publication PostgreSQL Replication - Second Edition \cite{pg} provides an overview of replication types, which are summarized in following paragraphs.

We can distinguish between \textit{single-master} and  \textit{multi-master} replication. More common and traditional way of database cluster design is to have just a single replica, which can handle write requests. The updates from master replica, processing writes is distributed to replicas, which operates in the read-only mode. In order to prevent overloading of master, the read requests can be redirected to slave (read-only) replicas.

The second option is to allow load balancing also write requests to all nodes in the cluster. The distribution of all the requests can be an advantage, however, this approach adds a lot of complexity to the solution, it is less transparent and some conflicts in data consistency can occur easily.

The second way of classification is to distinguish whether replication is synchronous or asynchronous. In case the transaction can be committed on master and copied to slave with a small delay, it is called asynchronous replication. Slave is usually a little bit behind, but transactions are fast and immediate. The synchronous replication is more strict in terms of consistency. The transaction can't be committed, before the update is copied to at least two nodes in the replicated cluster. This approach guaranties better consistency, but transactions take longer.

There are also various alternatives how to implement copying of the data between nodes. The concept of physical replication means, that the data are copied in binary format to other nodes. The content of master's data directory is replicated to slaves byte by byte. The logical replication in contrast, transfers logical changes based on the replication identity.

Physical replication is more mature and standard way of streaming data among nodes. It is usually easier to set up, and it works well for cases when scaling cluster size up and down is needed. The limitation is that typically the same version of software needs to be on all nodes.

Logical replication, on the other hand is more flexible, it allows to replicate just a selected tables and send different sets of changes to different subscribers. The setup of logical replication is usually more complex and fast scaling is harder. Logical replication is possible across the database versions, which can be beneficial especially for database upgrades.

\subsection{Failover} \label{sec:failover}
High availability of the database is one of the main motivations to use replicated database clusters as it was mentioned at the beginning of Section \ref{sec:db_replication}. There are multiple nodes keeping the current state of the data, which can be located in the different data centers. In case of the software or hardware failure on one of the nodes, there are still several other nodes, which holds the data and can handle incoming requests.

In case of the single-master replication, sometimes also called master-slave setup, there are two different disaster scenarios. When the defect occurs on one of the slave nodes, it can be restarted or turned off and replaced by a new node. The affected slave replica won't be accepting the data increments for some time, but it can pull all the changes and synchronize with the master node after it is recovered. The functionality and performance of the cluster in not affected too much in majority of the cases.

The worse situation is if the data corruption or some kind of software failure occurs on the master node. The usual solution of such scenario is to choose one of the healthy slave replicas and promote it to a new master. This replica starts handling write requests after promotion. All the other nodes, including the old master, in case it is available again, must be informed, which node is a new master. This procedure of promoting one of the slaves to a master node is called the failover.

It is not easy to set up the failover correctly. In case the promotion happens immediately after the current master becomes unavailable, some small network issue or delay can trigger the failover even when it is not necessary. On the other hand, the longer waiting period also prolongs resulting unavailability of the cluster. In case the communication quality between the nodes is not working well, it can happen that multiple nodes run in master mode at the same time. The properly configured and well working failover is basically the only way to achieve reasonable level of high availability in the single-master replication cluster.

\subsection{CAP theorem}
The CAP theorem \cite{cap}, visualized in Figure \ref{fig:cap} is a fundamental trade-off between concepts of \textit{consistency}, \textit{availability} and \textit{partition tolerance}. This idea was originally introduced by Eric Brewer in 2000 \cite{brewer_cap}.

The consistency requirement states that all nodes of distributed system see the same data at the same time. In the world of databases it basically means, that read operation on any node including read-only replicas returns the same value, which is the most recent one. A system is consistent if every transaction starts with the system in a consistent state and also ends in the consistent state.

Availability simply means, that read an write operations have to be successfully handled all the time. The system is not allowed to ignore client's requests.

The last requirement of the CAP theorem is the partition tolerance. Partition tolerance condition states that the system continues to work even in case some of the messages are delayed or lost on the way. The system can be partitioned into multiple groups, which can't communicate with each other. The system must be able to work correctly in such situation in order to be partition tolerant.

The core idea of CAP theorem is that it is theoretically not possible to offer all three conditions at the same time. The important point to think about, when designing a replicated system is which conditions are crucial in order to serve the purpose.

When designing the application supporting some business, calculating prices of the goods or services, the main priorities are the  consistency and partition tolerance. In case the application is for example collecting some data, the consistency is not critical, the data can be synchronized with some reasonable delay.
\begin{figure}[H]
\caption{CAP theorem visualization}
\centering
\includegraphics[width=0.8\textwidth]{CAP}
\label{fig:cap}
\end{figure}

\section{PostgreSQL} \label{sec:postgresql}
The initial implementation of POSTGRES project, based on the relational data model started in 1986 at the University of California at Berkeley according to the PostgreSQL history \cite{pg_history} documentation page. The original query language PostQUEL was replaced by standardized SQL few years later, which resulted in choosing new name PostgreSQL for the whole project. PostgreSQL is an open source database, developed and supported by an active community. It runs on all major operating systems and it is extensible thanks to the concept of PostgreSQL extensions \cite{pg_extensions}.

PostgreSQL supports storing and querying the data in JavaScript Object Notation (JSON) format besides many traditional SQL data types. It is also not limited to the pure relational model design. The inheritance mechanism in PostgreSQL \cite{inheritance} allows to extend schema of already defined tables. The inheritance is a concept used mainly in Object-Oriented Database Management Systems \cite{oodbms}. The documentation defines PostgreSQL as an object-relational database, because of the support for mentioned features on top of the pure relational database principles.

\section{Replication in PostgreSQL} \label{sec:pg_replication}
PostgreSQL provides native support for replication setup to create an high availability cluster \cite{pg_ha}. PostgreSQL uses slightly different terminology. Term \textit{primary} is used for master node and slave node is called \textit{standby}.

Besides the native replication, there are multiple tools and systems for high availability. These tools provide additional features and require less amount of manual work and configuration. Following paragraphs introduce some of the available solutions for High availability for PostgreSQL.

\subsection{Native replication}
The native PostgreSQL replication is sufficient for building full, production ready replicated cluster. It supports Warm Standby/Log Shipping high availability mode, which replicates the data to standby replicas, but they are not set up for handling queries. The standby replicas are only a backup, prepared to be promoted to master replica in case of a failover explained above. In version 9.0 the Hot Standby/Streaming Replication was added. The incoming read-only queries can be load balanced to standby replicas in the streaming replication mode and prevent overloading of the master node.

\subsection{Slony}
Slony-I \cite{slony} is an asynchronous replication system for PostgreSQL including the capabilities to replicate large databases. The design objectives were based on the analysis of existing replication systems at the time of the design. Some of the main goals were an extensibility of the replication system and the concept of cascading slave nodes. The idea of cascading is, that every node can forward the updates to other nodes. If slave nodes are able to send data to other slave replicas, it prevents overloading of the master node.

\subsection{Bucardo}
The use cases of Bucardo \cite{bucardo} replication program are similar to Slony-I. The core part of Bucardo is a daemon written in Perl \cite{perl} programming language.

The replication information needed for the daemon is stored in the main Bucardo database. This database contains list of databases involved in the replication, information about tables and replication mode. The daemon listens for the notify requests, connects to the remote databases and copies the data. The Bucardo daemon controlling the replication can be running on one of the servers involved in the replication or on the separate server. One of the advantages Bucardo provides is a multi-master replication mode.

\subsection{Repmgr} \label{sec:repmgr}
Repmgr \cite{repmgr} is an open source tool enhancing the PostgreSQL's built-in replication capabilities. Repmgr is a lightweight tool, implemented as a full PostgreSQL extension from version Repmgr 4.0. It simplifies the cluster initialization and wraps parts of the replication setup and operations into one line commands.
The first important  of Repmgr is a command-line tool serving for administrative actions like node setup, promoting of chosen slave to master and displaying cluster status.

Another component is a daemon called repmgrd \cite{repmgrd}. The administration can optionally run this management and monitoring daemon on each node in the replication cluster. Repmgrd is designed to automate many actions such as failover and pointing other slave to follow the new master node.

\subsection{Pglogical}
PostgreSQL version 10 introduced the initial support for logical replication, which is an alternative approach to the standard streaming (physical) replication in PostgreSQL. The difference between these two principles was explained in Section \ref{sec:types_of_replication}. The PostgreSQL extension called pglogical \cite{pglogical} was developed to utilize latest PostgreSQL features and supporting use cases like upgrades between major database versions, selective replication of sets of tables and data merge from multiple upstream servers.

\subsection{Pgpool-II} \label{sec:pgpool}
Even though the set of features provided by Pgpool-II \cite{pgpool} is overlapping with the previously explained tools, its design and purpose are different. Pgpool-II serves as a proxy, redirecting PostgreSQL database client requests to servers and resulting data back to the clients.

Connection pooling is the core feature of Pgpool-II. The established connections are reused more effectively thanks to this mechanism. Another useful feature is load balancing of read-only queries to multiple servers.

The replication itself and the failover can be handled on Pgpool-II level as well. The data are not streamed from master node to replicas, but they are backed-up to multiple servers by Pgpool-II sitting in front of the database servers.

\subsection{Summary}
Many different tools to handle high availability for PostgreSQL have been developed over the years. Theirs design goals and covered use cases are overlapping in many aspects. Available tools implement different replication approaches, and similar approaches are implemented in many different ways.

Each solution has its benefits and down sites as well. Introduced alternatives are analysed and compared in the following Chapter. Section \ref{sec:selected_technologies} provides an explanation, which technologies are the best fit for automated cluster designed for OpenShift environment.

\chapter{Replicated PostgreSQL in OpenShift} \label{chap:pg_in_openshift}
The data layer is an essential component of real world applications, especially in the business sector. Data, stored in stateful database systems are critical parts of the system and cannot be doubled or replaced as easily as stateless components. Running database systems in containerized way can be very uneasy. Scaling or restarts of the database nodes can lead to data integrity violations, or data loss if it isn't done carefully. The common practice is that applications running in the Kubernetes clusters alongside other stateless components are connected to the external database system. This database system is usually replicated (see Section \ref{sec:db_replication}), but it is not containerized and managed by the container management platform.

Combining containerized application layer and standalone data layer is working sufficiently for many use cases. However the possibility to treat the data layer the same way as all the rest brings many benefits. The same tools can be used to monitor and manage all the layers. High availability and automation provided by Kubernetes can simplify maintenance of database systems as well.

On the other hand some of the Kubernetes self-healing capabilities bring higher probability of database restart or failover (see Section \ref{sec:failover}) even in case it is not necessary to keep the database cluster running. Running database clusters in Kubernetes is not widely adopted so far, there is a room for improvement and research.

This Chapter covers objectives for highly available cluster, technologies selected for particular components, and an important parts of the cluster API, design and implementation.

\section{Objectives}
The main objective of this thesis is to design and implement replicated PostgreSQL cluster composed of multiple kinds of container images. The cluster setup, monitoring and management should require minimal effort. The main responsibilities of the automated solution are to properly initialize PostgreSQL nodes, keep them available and healthy and trigger the failover when the master node stops working correctly.

The first step is to explore existing solutions and tools on both sides, PostgreSQL replication and managing stateful applications in Kubernetes and OpenShift. Tools having attributes, which satisfy the requirements of automated, containerized environment need to be identified. After the right tools are selected, they must be containerized and prepared for running in the OpenShift environment.

The core component, which needs to be designed and implemented is the operator bringing all the pieces together, monitoring the cluster, exposing the API for its management and ensuring the correct behavior and high availability.

Preparation of the tests simulating crashes in the real production environment and testing the solution is the final step. Cluster ability to initialize the cluster, scale number of replicas and handle the failover should be covered in the test scenarios.

\section{Selected technologies} \label{sec:selected_technologies}
Operator Framework, introduced in Section \ref{sec:operator_framework} is basically the only option to write Kubernetes operators for stateful applications. The framework provides three variants of the operator, pure Go operator, Ansible and Helm operator. Writing business logic of the operator directly in Go programming language is the most mature option and it brings the best flexibility and customization possibilities in the operator design.

There are many options and tools to set up PostgreSQL replication. Each type of replication have different properties and advantages (see Section \ref{sec:pg_replication}).

The most important attributes of fully automated solution running in OpenShift are easy setup, possibility to scale number of replicas and ability to survive restart of any database node. The light weight solutions are preferred, in order to keep size of the container images reasonably small. The master-slave replication is generally more standard and less complex approach, compared to multi-master model, also the selection of tools supporting master-slave is much larger.

The concept of logical replication (see Section \ref{sec:types_of_replication}) brings many benefits, like easier upgrades, better flexibility and replication of selected database subset. However, easy setup and fast scaling of the physical replication are critical for correctly working setup in OpenShift environment. The implementation of logical replication in PostgreSQL is also relatively new and it lacks working examples and documentation.

The asynchronous, physical replication based on master-slave model is the best fit for automated solution, running in Linux containers (see Section \ref{sec:containers}). When the advantage of light weight design, easy setup and management in the automated way is taken into an account Repmgr \ref{sec:repmgr} is the most logical choice. Repmgr is implemented as PostgreSQL extension installed alongside standard PostgreSQL server. It is easy to set up and it automates cluster operations including failover thanks to repmgrd.

For connection pooling and load balancing of read-only requests in PostgreSQL, Pgpool-II (see Section \ref{sec:pgpool} is the only mature technology available.

The final list of technologies selected for implementation of high availability for PostgreSQL in automated OpenShift environment are Repmgr extension for replication setup and monitoring, repmgrd deamon performing the failover in automated way, Pgpool-II serving as a proxy for connection pooling and load balancing and pure Go operator exposing the cluster API and managing all the Kubernetes objects, which encapsulate all the components.

\section{Cluster design}
There are multiple design goals, which needs to be achieved to ensure high availability and make the solution capable of running in OpenShift environment. To achieve high availability, not only replication on the database system level needs to be set up, but every potential single point of failure must be eliminated from the cluster.

Another important goal is making every component except storage replaceable, able to load the configuration and start automatically during first initialization and after restart as well. The incoming requests should be distributed across multiple pods to prevent overloading of some cluster components.

The high level design of the cluster implementing high availability for PostgreSQL in OpenShift environment is visualized in Figure \ref{fig:cluster_design}.

\begin{figure}[H]
\caption{Design of PostgreSQL cluster in OpenShift}
\centering
\includegraphics[width=1\textwidth]{cluster-design}
\label{fig:cluster_design}
\end{figure}

There are multiple Linux containers containing PostgreSQL server with Repmgr extension. One container is running as the master node and the rest of them are slaves. The slave nodes are running in \textit{hot standby} mode.

Each node is backed by persistent storage. The storage in Kubernetes is an abstract term, the administrator can choose from large selection of storage provisioners. Provisioner defines its API and configuration options.  The PostgreSQL cluster implementation doesn't limit this abstract approach in any way. The administrator is able to plug in the storage provisioner of choice.

One Kubernetes service (see Section \ref{sec:service}) serves as a proxy for current master. This primary service provides stable domain name for write queries. In case the master replica is restarted IP address of the pod is changed, service stays in place and a master replica is still accessible using domain name of primary service. When the failover occurs, the new master pod has a different name and it is created and managed by different deployment. Operator logic detects failover in underlying cluster and changes primary service parameters to make it redirect write queries to the new master.

The second service is load balancing incoming requests to all the PostgreSQL nodes in the cluster. Slaves are running in hot standby mode, so this service can be used for read-only queries to prevent overloading of master replica.

Pgpool-II containers are used as a proxy. The replication and failover are handled by Repmgr installed inside PostgreSQL container, these features are disabled in Pgpool-II containers. The proxies are configured to load balance read-only queries to the read-only service. Pgpool-II load balancing is session based, which means that consequent requests from single client goes to the same database node. This approach minimizes the potential issues related to asynchronous replication.

The number of database nodes, Pgpool-II replicas and many other configuration details of the cluster can be adjusted by the administrator using cluster API.

\section{PostgreSQL container images} \label{sec:pg_cnt_images}
The container images for replicated cluster need to contain PostgreSQL server, Repmgr extension and some additional tooling to deliver high availability. The second requirement is to satisfy all the OpenShift security policies and be able to run in the cloud environment without too much manual configuration required.

Building similar image from scratch would require a lot of work and effort. One of the design goals of container images, explained in Section \ref{sec:containers} is to make existing artifacts extensible.

PostgreSQL container images \cite{pg_cnt} are an open source collection of images designed to run in OpenShift or plain Kubernetes. This repository supports multiple stable versions of PostgreSQL and each of them based on RHEL \cite{rhel}, CentOS \cite{centos} and Fedora \cite{getfedora} operating systems.

Both RHEL and CentOS are the long term support distributions, they guarantee the API stability for long time period, so there is not an easy way to add new packages or features to these distributinos in a reasonable time frame. Fedora, on the other hand is a community distribution opened to the latest software of different kinds. A new version of Fedora is released twice a year.

Fedora based image from PostgreSQL container images project is available with complete installation of PostgreSQL version 10 and it is running in OpenShift very well. The only missing piece of software is the Repmgr extension.

Repmgr was not available in the standard RPM \cite{rpm} format, supported by Fedora distribution. It was necessary to build, test and create a new Fedora package for Repmgr \cite{fedora_repmgr} as one of the first steps of implementing high availability for PostgreSQL in OpenShift. The guidelines for making new RPM packages \cite{fedora_new_package} were followed.

After the package was approved and added to Fedora repositories \cite{fedora_repositories} it was necessary to add this package to existing Fedora based PostgreSQL 10 container image \cite{pg_img}.

PostgreSQL container image also provides a mechanism for image extending \cite{pg_cnt_extending}. Custom initialization and configuration scripts, enabling the Repmgr extension can be added to the image using this feature.

Pgpool-II was available in RPM format, so it was enough to add this package to the Fedora base image for OpenShift, add a couple of configuration and initialization scripts and build images, which can run in front of the replicated PostgreSQL nodes and serve for connection pooling and load balancing.

All three components, spec file for Repmgr RPM package, PostgreSQl container scripts and Pgpool-II container definition are included in the List of electronic appendices \ref{chap:appendices}.

\section{PostgreSQL Operator}
PostgreSQl operator is the core part of highly available cluster implementation. It controls state of the PostgreSQL pods and contains the logic keeping the cluster healthy and up to date with the latest desired state.

The operator is based on the latest template for Go operators generated by Operator SDK, explained in Section \ref{sec:operator_framework}. The main parts of the operator are the API, explained in more details in the following subsection, internal representation of the PostgreSQL nodes, control loop logic and handlers managing underlying fundamental Kubernetes objects.

When the reconcile loop is triggered, the operator control logic iterates over map of all desired nodes, defined via operator API and ensures the current nodes running in the cluster reflects the desired state. In case there are some differences, nodes in the cluster are created, removed or modified according to the current specification.

\subsection{Operator API} \label{sec:operator_api}
The Operator Framework provides a mechanism to create a Custom Resource definition. Custom Resource has its own API in form of the Kubernetes manifest (see Section \ref{sec:manifest}). This manifest contains the usual \textit{apiVersion}, \textit{metadata} and \textit{spec} fields. The structure, semantics and allowed values under spec field are defined by the Custom Resource author. The manifests content is parsed by the operator each time the reconciliation loop is triggered and the operator logic can perform changes to the cluster based on its latest specification.

The first attempt to define an API for PostgreSQL is based on a simplistic design. It is providing common node specification for whole cluster:

\begin{lstlisting}
apiVersion: postgresql.openshift.io/v1
kind: PostgreSQL
metadata:
  name: postgresql-cluster
spec:
  managementState: managed
  size: 3
  nodeSpec:
    image: "mcyprian/postgresql-10-fedora29:1.2"
    resources:
      limits:
        memory: 1Gi
        cpu: "750m"
      requests:
        memory: 512Mi
        cpu: "250m"
    storage:
      emptyDir: {}
\end{lstlisting}

The \textit{managementState} field defines, whether the operator is active or not. There are situations, when administrator needs to perform some operation on the cluster and don't want the operator to touch the cluster or modify its state. If the management state is set to \textit{unmanaged}, operators control loop is an empty operation.

Size attribute specifies how many nodes should be run in the cluster. As it is an implementation of master-slave replication \textit{size: 3} means, there will be a single master and two slave replicas.

Image field serves for specifying the container image used for the PostgreSQL nodes. Minor version update can be performed by changing the image tag to higher number.

The \textit{resources} reflects the standard way of deployment resources definition in Kubernetes as explained in Section \ref{sec:resources}.

Similarly to resources, the \textit{storage} field can contain the storage provisioner of different kinds and settings specific to selected provisioner. The storage abstraction in Kubernetes was explained in the Persistent volumes Section \ref{sec:pv}.

This initial API design is simple and straightforward. A few lines of manifest are enough to define a replicated PostgreSQL cluster of desired size. However, when considering practical scenarios of the cluster administration, this design doesn't provide much flexibility. In case the administrator plans a minor update of the cluster, he has to update all the nodes, including master node at once. It also doesn't allow to identify the slave nodes and try some different experimental kind of storage without affecting master node.

The final operator API design is slightly more complex but it gives the administrator more flexibility in the cluster definition. Each node is identified by its name and the separate configuration for each of them can be given:

\begin{lstlisting}
apiVersion: postgresql.openshift.io/v1
kind: PostgreSQL
metadata:
  name: postgresql-cluster
spec:
  managementState: managed
  nodes:
    node-one:
      image: mcyprian/postgresql-10-fedora29:1.2
      priority: 100
      resources:
        limits:
          memory: 1Gi
          cpu: "750m"
        requests:
          memory: 512Mi
          cpu: "250m"
      storage:
        storageClassName: local-storage
        size: 20Gi
    node-two:
      image: mcyprian/postgresql-10-fedora29:1.2
      priority: 60
      resources:
        limits:
          memory: 1Gi
          cpu: "750m"
        requests:
          memory: 512Mi
          cpu: "250m"
      storage:
        storageClassName: local-storage
        size: 20Gi
    node-experimental:
      image: mcyprian/postgresql-10-fedora29:latest
      priority: 0
      storage:
        storageClassName: local-storage
        size: 20Gi
\end{lstlisting}

This solution defines nodes as a key value map. Each node can have specific resource requests and limits, it can be using different storage provisioner and storage settings and can use different container image. An additional \textit{priority} field is added to define node priority to became master when the failover occurs (see Section \ref{sec:failover}). In case the administrator wants to experiment with some of the slave nodes, its priority can be set to zero to make sure this node won't be promoted to master anyhow.

\subsection{Internal representation of nodes}
 Each node defined in the operator API nodes map is represented as a Go structure called \textit{deploymentNode} in the operator logic. The operator creates a Kubernetes deployment (see Section \ref{sec:deployment}) creating a single pod. Cluster name and namespace are propagated into the deployment definition. There is also one Kubernetes service created for each node to have a stable domain name for internal access within the cluster.

 Go structure contains references to the deployment and service structures of the node and connection to its database. The database connection serves mainly to retrieve current replication status from inside the pod. Alongside the current status, the deploymentNode structure provides methods to create, update and delete the node.

 The operator control loop works with the map of deploymentNode structures and executes theirs methods to make cluster reflect the desired state.

\section{Cluster initialization}
Fedora based PostgreSQL container images explained in Section \ref{sec:pg_cnt_images} contains specific scripts for initialization of master and slave replica. When the cluster is initialized for the first time, operator selects the master node from the nodes map specification based on the priority field. Master initialization script is then executed on this node. Configuration files for PostgreSQL and Repmgr extension are generated. The operator passes a set of environment variables to the initialization scripts. Configurable attributes in the configuration files are set accordingly. PostgreSQL server is started and Repmgr command to register master node is executed followed by starting the repmgrd daemon. The primary service exposing master replica to the clients is created before cluster initialization and it starts redirecting requests to the master node as soon as it is initialized.

The initialization process of standby nodes is slightly different. Configuration is generated as well, after that slave nodes tries to connect to the master node and starts cloning content of its PostgreSQL data directory. Cloning command is repeated on failure, because the master replica needs to be fully initialized before it can succeed. Operator starts nodes with small delay, so the slave nodes usually waits for master replica being ready. After the data are cloned from master node, PostgreSQL server is started, Repmgr register command is run to register the node as slave and repmgrd daemon is started as well.

In case some node is initialized after restart and its data directory is already in place, the current role of the node is detected and the corresponding initialization script is executed. Configuration files stored under PostgreSQL data directory, which is mounted to the container as a persistent volume survives pod restarts, configuration living in other locations is regenerated each time the new container is initialized.

\section{Database interaction}
The operators written in pure Go are usually using Go library to interact with Kubernetes API \cite{k8s-api} for cluster management. This robust API provides calls to read and modify all kinds of Kubernetes objects inside the cluster. The Kubernetes API can be used to get a pod, get its containers specification and metadata and also to modify container resources, startup command, environment variables and many other attributes. However, its not possible to access the file system and processes inside containers using the API. The usual way to read or debug containers file system is executing shell commands inside running container and parsing the output.

Repmgr stores the cluster information in the PostgreSQL database system itself. Each node contains replication database called \text{repmgr} storing important data about all the nodes, roles and accessibility. The table storing data of simple replicated cluster with one master node and three slaves looks similar to an example in Table \ref{table:repmgr_nodes}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c c c c c|}
 \hline
 node\_id & active & node\_name & type & priority \\ [0.5ex]
 \hline
 1 & t & node-one & primary & 100 \\

 2 & t & node-two & standby & 60 \\

 3 & t & node-three & standby & 30 \\

 4 & t & node-extra & standby & 0 \\ [1ex]
 \hline
\end{tabular}
\caption{Example of cluster representation in \textit{repmgr.nodes} table.}
\label{table:repmgr_nodes}
\end{table}


The operator needs to reflect current status of Repmgr nodes in configuration of services. Also, these data needs to be exposed to the administrator in operator status.

Interaction with the repmgr databases inside PostgreSQL pods is based on database connections. Go provides package for handling PostgreSQL connections, queries and storing the results in the native structures of the language. Constructor of the deploymentNode structure, explained in previous section opens the connection to PostgreSQL server running inside container. This database connection is stored alongside corresponding Kubernetes deployment and service in the deploymentNode structure. Database queries are an efficient way to get latest information about the replicated cluster nodes.

\section{Failover implementation}
The failover principle was described in Section \ref{sec:failover}. When a serious error occurs on the master node in single-master replication, one of the slave replicas is promoted to new master. The repmgrd daemon running on each PostgreSQL replica is monitoring status and availability of other database nodes. The failover process is automated by repmgrd, so it is not necessary to solve problems like detecting of master node unavailability and election of new master in the operator logic.

In case the current master replica becomes unavailable and the connnection is not recovered within specified time limit the process of failover is started automatically by repmgrd. Node priorities to become master from the cluster manifest are propagated to Repmgr configuration and taken into an account by repmgrd. One of the available slave replicas is chosen and promoted to master. The rest of nodes are switched to follow the new master. When failover occurs the operator detects the event. It modifies primary service to expose the new master node and it updates node roles in the cluster status.

\section{Proxy implementation}
As it was mentioned in Section \ref{sec:selected_technologies}, the replication and failover modes are disabled in Pgpool-II containers, they serve only for connection pooling and load balancing. The replicated PostgreSQL managed by the operator can be used also without Pgpool-II layer. The only issue is that read-only service is hardly usable, but replication and failover are independent from the proxies. As the PostgreSQL replication layer and proxy layer are loosely coupled, it is not necessary to manage Pgpool-II components from the operator. There are separate Kubernetes manifests created for Pgpool-II deployment and service that can be adjusted by the administrator and used according to his needs.

\chapter{Testing} \label{chap:testing}
One of the thesis objectives is to verify correctness of the solution by simulating crashes and testing behavior of the cluster. There are many testing methods, so testing can be done on different levels. The important functions and components of the PostgreSQL operator are covered by unit tests, but this chapter focuses mainly on testing of the whole solution from the start to the end. The end-to-end testing can discover issues in the operator logic, PostgreSQL container scripts, proxies and in the integration of these components.

Continuous integration (CI) and delivery is a very common practice in the Cloud Native world. If the tests can be fully automated it is a big advantage. On the other hand, automation of the testing process sometimes requires isolating some components and mocking out the rest of the system.

The highly available PostgreSQL cluster was tested in two phases. The result of first phase is a set of fully automated tests of the cluster designed to run in CI. Testing of the full setup, including all the components from the database system nodes to the client application with different settings applied was done manually as a separate step. Following sections explain both phases in more details and summarize the test results.

\section{Automated end-to-end testing}
The OpenShift environment is quite specific, proper end-to-end test can be done only inside the real OpenShift cluster. The Operator framework (see Section \ref{sec:operator_framework}) provides an end-to-end test framework to ensure the operator works as intended in real-world scenarios. Each test defines test cluster, which is created in OpenShift in a temporary namespace. The cluster can be modified and multiple assertion of Kubernets objects and theirs state can be specified. All the objects and the namespace are deleted after the test is finished. Minishift or Minikube (see Section \ref{sec:minikube}) are ideal tools for running similar tests locally or in the CI.

The first automated test focuses on scaling of the nodes. A small cluster composed of one master and one slave node is initialized. When the initialization is complete and the cluster returns correct status, slave is removed from cluster specification and the change is applied to the cluster. End-to-end test utilities provide \textit{WaitForDeletion} call. This is used to verify that slave node was successfuly removed from the cluster. Assertion of the cluster status, reporting single master node follows.  Slave node is upscaled again and the final check ensures that the node was registered as a slave and connected back to the cluster.

The second test checks cluster behavior when some of the nodes are restarted, which can happen relatively often in the OpenShift environment. The cluster of size two is initialized, slave node is restarted and the cluster status is checked, when the slave node is up again. The same is done for master node afterwards. This scenario ensures that failover is not triggered, when the master node is simply restarted and becomes available in a short period of time.

The failover test, simulates failover scenario in a small cluster. Initialization is the same as in the two previous scenarios. The master node is downscaled and it stops communicating with the slave. The assertion checks that failover is triggered after some time and the slave node is promoted to new master.

These tests verify the basic behavior of the operator and logic inside containers. It can be run in CI and it can help discover regressions when some changes are applied to the operator.

The source code of tests described in this Section can be found under \textit{postgresql-operator/test/e2e/} directory inside \textit{postgresql-operator.zip} archive, which is included in the electronic appendices \ref{chap:appendices}.

\section{Manual failover testing}
The goal of manual failover testing is to simulate the crash of master node and check if the failover works correctly. The first requirement for proper testing is to prepare an environment, which is as close to the real production cloud environment as possible, The test verifies that the master node misbehavior is detected, the slave with the highest priority is selected for promotion, and the downtime is reasonably short.

The test environment is built inside Minishift cluster and consists of following components:
\begin{itemize}
  \item PostgresSQL cluster - a PostgreSQL cluster managed by the operator;
  \item Pgpool-II deployment - Pgpool-II proxy containers;
  \item Test servers - two replicas of a simple web based application;
\end{itemize}

A test user and database are created on the PostgreSQL nodes. The test database contains single table, which stores the timestamps. Pgpool-II deployment is configured to redirect read-only queries to the read-only PostgreSQL service and write queries to the primary service.

The test server is a simple application providing two routes: the first one serves to write current timestamp to the database, and the second one reads and returns the last stored timestamp. Two replicas of the test server are deployed to the same Minishift cluster and test database credentials are injected into them. The test servers are connected to Pgpool-II service not to the PostgreSQL nodes directly. Forwarding of the local ports to the test server pods is set up. The two local scripts periodically send requests to both routes and log the responses. One script is called \textit{writer}, it calls route to store current timestamp every second in an infinite loop. The second one, called \textit{reader}, works similarly, but it just retrieves and log the last stored timestamp.

The full failover test scenario is following:
\begin{enumerate}
  \item  A fresh test environment is initialized. 
  \item Local ports are forwarded to test server pods.
  \item A \textit{test\_timestamps} table is created in test database.
  \item The reader and writer scripts are started.
  \item Corruption of the volume mounted to primary node is simulated.
  \item The failover process is monitored, until writes to the new master start to succeed.
  \item The downtime is measured based on the reader and writer logs.
\end{enumerate}

When the master node stops responding, slaves try to reconnect multiple times before the failover is triggered. Repmgrd (see Section \ref{sec:repmgr}), allows configuring the number of attempts to reconnect via \textit{reconnect\_attempts} parameter and the delay between attempts using the \textit{reconnect\_interval} parameter. The longer reconnect interval means that master node will be unavailable for longer time before the failover is triggered. On the other hand in case the interval is too short the minor network issue or restart of the master node can trigger the failover even when it is not really necessary. The total downtime during failover test is a sum of the reconnect interval multiplied by number of attempts, selection of the new primary node, promotion of selected slave to a new primary and update of the primary service.

The test scenario was repeated multiple times with different cluster sizes and reconnect interval settings. The test results are summarized in Table \ref{table:failover_testing}.

\begin{table}[ht!]
\centering
\begin{tabular}{|c c c c|}
 \hline
 cluster size & reconnect interval & write downtime & read downtime \\ [0.5ex]
 \hline
 2 & 6 x 10 s & 66 s & 4 s \\
 3 & 6 x 10 s & 71 s & 5 s \\
 2 & 4 x 10 s & 48 s & 0 s \\
 3 & 4 x 10 s & 52 s & 0 s \\
 2 & 4 x 7 s  & 40 s & 0 s  \\
 3 & 4 x 7 s  & 43 s & 4 s  \\ [1ex]
 \hline
\end{tabular}
\caption{Service downtime during failover}
\label{table:failover_testing}
\end{table}

The results of failover testing proof high availability of the cluster. It is able to failover autonomously in case of serious crash or disk corruption on the master node.

The read queries are redirected to read-only service, which load balances requests to all the nodes including master. When the query is redirected to the master node during its unavailability, it fails after some connection timeout.
The read timeout of a few seconds, which appeared several times is caused by the database connection timeout and one second delay between the attempts. The write downtime is under one minute if the correct settings are used. The number of slave nodes in the cluster has minimal impact on the downtime duration.

An issue related to repmgrd behavior was discovered when testing the cluster of three nodes. When the master node crashes, failover is triggered and one of the slaves becomes the new master. The other slave is attached to the new master node and PostgreSQL server is restarted. After the restart, slave is registered to Repmgr cluster, data are correctly replicated from the new master, but the repmgrd daemon attempts to connect to the old master node, which is unavailable. Switching the upstream node of all slaves properly after the failover should be the responsibility of Repmgr. This issue was reported to the Repmgr project on GitHub \cite{repmgrd_issue}.

\chapter{Conclusion} \label{chap:conclusion}
The main objective of this thesis was to design and implement a PostgreSQL cluster that provides high availability and runs in the cloud environment with minimal effort required from the administrator.

Many existing alternatives for PostgreSQL replication were compared and analysed. It was necessary to identify tools, which satisfy requirements of the cloud environment like easy setup, scaling and ability to survive restarts.

The solution consists of the containerized PostgreSQL nodes with Repmgr extension backed by a persistent storage, Pgpool-II containers serving as a proxy and the PostgreSQL operator. The high availability is achieved by streaming replication and automatic failover, which is triggered if a master node crashes. The PostgreSQL operator is designed to initialize, manage and monitor the replicated cluster. It exposes an API, which allows defining a replicated cluster of the desired size using a single Kubernetes manifest.

The ability of autonomous cluster setup, scaling and failover was proved by the tests simulating crashes in the Openshift environment. The downtime during failover can be reduced to less than one minute.

The cluster API is highly extensible. If it turns out that some of the PostgreSQL or Repmgr settings need to be often adjusted by cluster users, it can be added as a new field to the manifest and propagated to containers by the operator.

Major PostgreSQL version upgrade can be problematic, especially in the systems based on physical replication. The major upgrade is not covered by the operator, however, it makes the cluster initialization trivial. An empty cluster using the new PostgreSQL version can be initialized alongside the original one and logical replication can be set up to synchronize the data to the new master node. Full support for a similar scenario can be potentially also integrated into the operator.

Running databases inside Kubernetes and Openshift is not easy. However, if the right tools are used and the operator is managing all the resources, it can be done efficiently and bring benefits to the administrators. Running different kinds of databases in the fully automated cloud environments alongside applications might become the standard solution in the near future.


\appendix %% Start the appendices.
\chapter{List of electronic appendices} \label{chap:appendices}
Electronic appendices contain source codes, container image definitions and other software components implemented as part of the highly available solution for PostgreSQL running in OpenShift:
\begin{itemize}
  \item postgresql-operator.zip - source code of the PostgreSQL operator
  \item postgresql-container.zip - Fedora based PostgreSQL container image, containing Repmgr extension and shell scripts
  \item pgpool-fedora-container.zip - Fedora based Pgpool-II container image and corresponding Kubernetes manifests
  \item repmgr.zip - Repmgr RPM package source files
\end{itemize}

\end{document}
